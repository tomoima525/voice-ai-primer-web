<!DOCTYPE html>
<html lang="ja">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Voice AI &amp; Voice Agents | 図解プライマー</title>
  <meta name="description" content="2025年の音声AIに関する総合ガイド">
  <meta property="og:title" content="Voice AI&amp; Voice Agents | 図解プライマー">
  <meta property="og:description" content="2025年の音声AIに関する総合ガイド">
  <meta property="og:image" content="images/meta.jpg">
  <meta property="og:type" content="website">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Voice AI&amp; Voice Agents | 図解プライマー">
  <meta name="twitter:description" content="2025年の音声AIに関する総合ガイド">
  <meta name="twitter:image" content="images/meta.jpg">
  <link rel="stylesheet" href="../styles.css">
  <link rel="icon" href="../images/favicon.ico">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
  <link href="https://fonts.googleapis.com/css2?family=DM+Sans:ital,wght@0,400;0,500;0,700;1,400&amp;display=swap"
    rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=DM+Mono:wght@400;500&amp;display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://use.typekit.net/rff5lbb.css">
  <link rel="stylesheet" href="../script/binary-numbers.css">
  <style>
    #table-of-contents ul {
      list-style-type: none;
      padding-left: 20px;
    }

    /* Japanese font configuration */
    body {
      font-family: "Yu Gothic", "Hiragino Kaku Gothic ProN", "Meiryo", "Noto Sans CJK JP", sans-serif;
      font-size: 1rem;
    }

    /* Ensure proper font rendering for Japanese characters */
    body * {
      font-family: inherit;
      -webkit-font-feature-settings: "kern" 1;
      font-feature-settings: "kern" 1;
      text-rendering: optimizeLegibility;
    }

    /* Smaller title font size for Japanese */
    .title {
      font-size: 3.75rem !important;
    }
  </style>
</head>
  <div style="position: absolute; top: 1rem; right: 2rem;">
    <a href="/" style="color: var(--foreground); text-decoration: none; font-weight: 500;">英語</a>
  </div>
<body>
  <div class="container">
    <div style="float: right; margin: 1rem 0 1rem 1rem; width: 10rem;">
      <img src="../images/Figure 0100 horizontal.svg">
    </div>


    <header class="header">
      <h1 class="title">音声AI &amp; 音声エージェント</h1>
      <h2 class="subtitle">図解入門</h2>
    </header>


    <nav id="table-of-contents">
      <h2 class="table-of-contents-title">目次</h2>
      <ol>
        <li><a href="#conversational-voice-ai">2025年の会話型音声AI</a></li>
        <li><a href="#about-this-guide">このガイドについて</a></li>
        <li><a href="#basic-loop">会話型AIの基本的なフロー</a></li>
        <li><a href="#core-tech">コア技術とベストプラクティス</a>
          <ul>
            <li><a href="#latency">4.1. レイテンシ</a></li>
            <li>
              <a href="#llms-for-voice">4.2. 音声ユースケース向けのLLM</a>
              <ul>
                <li><a href="#latency-llm">4.2.1. レイテンシ</a></li>
                <li><a href="#cost-comparison">4.2.2. コスト比較</a></li>
                <li><a href="#open-source">4.2.3. オープンソース / オープンウェイト</a></li>
                <li><a href="#speech-to-speech">4.2.4. speech-to-speech(音声→音声)モデルはどうなの？</a></li>
              </ul>
            </li>
            <li>
              <a href="#speech-to-text">4.3. Speech-to-text</a>
              <ul>
                <li><a href="#deepgram-and-gladia">4.3.1. Deepgram と Gladia</a></li>
                <li><a href="#prompting-help">4.3.2. プロンプトはLLMを助ける</a></li>
                <li><a href="#other-stt-options">4.3.3. その他のSTTの選択肢</a></li>
                <li><a href="#gemini-transcribing">4.3.4. Google Geminiでの文字起こし</a></li>
              </ul>
            </li>
            <li>
              <a href="#text-to-speech">4.4. Text-to-speech</a>
            </li>
            <li>
              <a href="#audio-processing">4.5. オーディオ処理</a>
              <ul>
                <li><a href="#avoiding-spurious-interruptions">4.5.1. マイクとゲインの自動制御</a></li>
                <li><a href="#echo-cancellation">4.5.2. エコーキャンセレーション</a></li>
                <li><a href="#noise-suppression">4.5.3. ノイズ抑制、音声、音楽</a></li>
                <li><a href="#encoding">4.5.4. エンコーディング</a></li>
                <li><a href="#server-side-noise">4.5.5. サーバーサイドノイズ処理と話者分離</a></li>
                <li><a href="#voice-activity-detection">4.5.6. 音声活動検出（VAD）</a></li>
              </ul>
            </li>
            <li>
              <a href="#network-transport">4.6. ネットワーク伝送</a>
              <ul>
                <li><a href="#websockets-webrtc">4.6.1. WebSockets と WebRTC</a></li>
                <li><a href="#http">4.6.2. HTTP</a></li>
                <li><a href="#quic-moq">4.6.3. QUIC と MoQ</a></li>
                <li><a href="#network-routing">4.6.4. ネットワークルーティング</a></li>
              </ul>
            </li>
            <li>
              <a href="#turn-detection">4.7. 発話ターン検出</a>
              <ul>
                <li><a href="#voice-activity-detection-4-7">4.7.1. 音声活動検出</a></li>
                <li><a href="#push-to-talk">4.7.2. プッシュトゥトーク</a></li>
                <li><a href="#endpoint-markers">4.7.3. エンドポイントマーカー</a></li>
                <li><a href="#context-aware-turn-detection">4.7.4. コンテキストを理解したターン検出</a></li>
              </ul>
            </li>
            <li>
              <a href="#interruption-handling">4.8. 割り込み処理</a>
              <ul>
                <li><a href="#avoiding-spurious-interruptions">4.8.1. 誤検知による不要な割り込みの回避</a></li>
                <li><a href="#maintaining-accurate-context">4.8.2. 割り込み後の正確なコンテキストの維持</a></li>
              </ul>
            </li>
            <li>
              <a href="#managing-conversation-context">4.9. 会話コンテキストの管理</a>
              <ul>
                <li><a href="#differences-between-llm-apis">4.9.1. LLM APIによる違い</a></li>
                <li><a href="#modifying-context-between-turns">4.9.2. ターン間でのコンテキストの変更</a></li>
              </ul>
            </li>
            <li>
              <a href="#function-calling">4.10. Function Calling</a>
              <ul>
                <li><a href="#function-calling-reliability">4.10.1. 音声AIにおける Function Callingの信頼性</a></li>
                <li><a href="#latency-function-calls">4.10.2. Function Callingのレイテンシ</a></li>
                <li><a href="#handling-interruptions">4.10.3. 割り込みの扱い</a></li>
                <li><a href="#streaming-mode">4.10.4. ストリーミングモードとFunction Callingのチャンク</a></li>
                <li><a href="#execute-function-calls">4.10.5. Function Callingをどのように、どこで実行するか</a></li>
                <li><a href="#async-function-calls">4.10.6. 非同期Function Calling</a></li>
                <li><a href="#parallel-composite-function-calling">4.10.7. 並列および複合的なFunction Calling</a></li>
              </ul>
            </li>
            <li><a href="#multimodality">4.11. マルチモーダリティ</a></li>
          </ul>
        </li>
        <li><a href="#multiple-models">複数のAIモデルの利用</a>
          <ul>
            <li><a href="#fine-tuned-models">5.1. 複数のファインチューニング済みモデルを利用する</a></li>
            <li><a href="#async-inference-tasks">5.2. 非同期推論タスクの実行</a></li>
            <li><a href="#content-guardrails">5.3. コンテンツガードレール</a></li>
            <li><a href="#single-inference-actions">5.4. 単一推論アクションの実行</a></li>
            <li><a href="#self-improving-systems">5.5. 自己改善システムに向けて</a></li>
          </ul>
        </li>
        <li><a href="#scripting">スクリプティングと指示の追従</a></li>
        <li><a href="#evals">音声AIの評価（Evals）</a>
          <ul>
            <li><a href="#evals-different">7.1. 音声AIの評価はソフトウェアの単体テストとは異なる</a></li>
            <li><a href="#failure-modes">7.2. フェイルモード</a></li>
            <li><a href="#eval-strategy">7.3. 評価戦略の策定</a></li>
          </ul>
        </li>
        <li><a href="#telephony">電話基盤との統合</a></li>
        <li><a href="#rag-memory">RAG（Retrieval-Augmented Generation）とメモリ</a></li>
        <li><a href="#hosting">ホスティングとスケーリング</a>
          <ul>
            <li><a href="#hosting-architecture">10.1. アーキテクチャ </a></li>
            <li><a href="#hosting-cost">10.2. 分単位コストの計算</a></li>
          </ul>
        </li>
        <li><a href="#future">2025年に向けての展望</a></li>
        <li><a href="#contributors">寄稿者</a></li>
      </ol>
    </nav>

    <main>
      <div class="chunk-row">
        <div class="chunk-content">

          <h1 id="conversational-voice-ai">1. 2025年の会話型音声AI</h1>

          <p>LLMは会話が得意です。</p>

          <p>ChatGPTやClaudeと自由形式の対話を多く行ったことがあるなら、LLMと話すことが非常に自然に感じられ、広範に有用であるという直感を持っているはずです。</p>

          <p>LLMは非構造化情報を構造化データに変換することも得意です。<sup>[1]</sup></p>

          <p>新しい音声AIエージェントは、これら二つのLLMの能力—会話と非構造化データからの構造抽出—を活用し、新しい種類のユーザー体験を作り出しています。</p>
        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[1] ここでは広義に意味しており、一部のLLMの「構造化出力」機能の狭義の意味に限定するものではありません。</p>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <p>音声AIは今日、さまざまなビジネス文脈で展開されています。例えば：</p>

          <ul class="arrow-list">
            <li>医療予約前の患者データ収集、</li>
            <li>インバウンドの営業リードへのフォローアップ、</li>
            <li>さまざまなコールセンター業務の処理、</li>
            <li>企業間のスケジュールや物流の調整、そして</li>
            <li>ほぼあらゆる種類の小規模事業の電話対応。</li>
          </ul>

          <p>消費者向けでも、会話型の音声（およびビデオ）AIはソーシャルアプリケーションやゲームにも浸透し始めています。開発者はGitHubやソーシャルメディアで毎日のように個人的な音声AIプロジェクトや実験を共有しています。
          </p>

        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h1 id="about-this-guide">2. 本ガイドについて</h1>

          <p>本ガイドは音声AIの最先端におけるスナップショットです。<sup>[2]</sup></p>

          <p>
            本番稼働に耐えうる音声エージェントを構築するのは難しいです。多くの要素はゼロから実装するのが容易ではありません。音声AIアプリを構築する場合、このガイドで扱う多くの事項についてフレームワークに依存することが多いでしょう。しかし、すべてをゼロから構築するかどうかに関わらず、部品がどのように組み合わさるかを理解することは有益だと考えています。
          </p>

          <p>このガイドはSean DuBoisのオープンソース書籍<a href="https://webrtcforthecurious.com" target="_blank">WebRTC For the
              Curious</a>に直接触発されました。その書籍は最初に公開されてから4年間、数多くの開発者がWebRTCを理解するのに役立ってきました。<sup>[3]</sup></p>

          <p>この文書における音声AIコード例は<a href="https://pipecat.ai"
              target="_blank">Pipecat</a>オープンソースフレームワークを使用しています。Pipecatはリアルタイム音声AIのためのベンダーに依存しないエージェント層です。<sup>[4]</sup>本書でPipecatを使用した理由は次のとおりです：
          </p>

          <ol class="list-decimal">
            <li>我々が日々これで構築し、メンテナンスをしていて、よく知っているからです！</li>
            <li>Pipecatは現在最も広く使われている音声AIフレームワークであり、NVIDIA、Google、AWS、OpenAI、および数百のスタートアップがPipecatを活用し、また貢献しています。</li>
          </ol>

          <p>この文書では一般的なアドバイスを提供するように努めており、商用製品やサービスを推奨することは避けています。特定のベンダーを挙げる場合は、多くの音声AI開発者がそれらを使用しているためです。</p>


          <p>それでは始めましょう …</p>

        </div>

        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[2] 本ガイドは元々2025年2月のAI Engineering Summit向けに執筆しました。2025年5月中旬に更新しています。</p>
            </div>
            <div class="footnote">
              <p>[3] <a href="https://webrtcforthecurious.com" target="_blank">webrtcforthecurious.com</a> —
                WebRTCは音声AIに関連があり、後述の<a href="#websockets-webrtc">WebSockets と WebRTC</a>の節で議論します。</p>
            </div>
            <div class="footnote">
              <p>[4] Pipecatは60以上の<a href="https://docs.pipecat.ai/server/services/supported-services"
                  target="_blank">AIモデルとサービス</a>の統合を持ち、ターン検出や割り込み処理などの最先端実装を備えています。Pipecatを使えば、WebSockets、WebRTC、HTTP、および電話を使ってユーザーと通信するコードを書けます。PipecatにはTwilio、Telnyx、LiveKit、Dailyなどを含むさまざまなインフラプラットフォーム向けのトランスポート実装があります。JavaScript、React、iOS、Android、およびC++向けの<a
                  href="https://docs.pipecat.ai/client/introduction" target="_blank">クライアント側Pipecat SDK</a>も用意されています。
              </p>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">

          <h1 id="basic-loop">3. 会話型AIの基本的なフロー</h1>

          <p>音声AIエージェントの基本的な「やるべき仕事」は、人間の発話を聞き、有用な方法で応答し、その順序を繰り返すことです。</p>

          <p>
            こんにちの本番環境の音声エージェントはほぼすべて非常に似たアーキテクチャを持っています。音声エージェントプログラムはクラウドで動作し、音声から音声へのループをオーケストレーションします。エージェントプログラムは複数のAIモデルを使用し、その一部はエージェントにローカルに実行され、他はAPI経由でアクセスされます。エージェントプログラムはまた、LLMのFunction
            Callingや構造化された出力を使用し、バックエンドシステムと連携します。</p>


          <ol class="list-decimal">
            <li>音声はユーザーのデバイスのマイクでキャプチャされ、エンコードされ、ネットワークを通じてクラウドで動作する音声エージェントプログラムに送信されます。</li>
            <li>入力音声は文字起こしされ、LLMへのテキスト入力が作られます。</li>
            <li>
              テキストはコンテキスト—プロンプト—として組み立てられ、LLMによって推論が行われます。推論の出力は多くの場合、エージェントプログラムのロジックによってフィルタリングまたは変換されます。<sup>[5]</sup>
            </li>
            <li>出力テキストは音声合成モデルに送られ、オーディオ出力が生成されます。</li>
            <li>生成された音声出力がユーザーに返送されます。</li>
          </ol>

          <p>
            音声エージェントプログラムがクラウドで実行され、音声合成(TTS:text-to-speech)、LLM、および音声認識（STT:speech-to-text）処理がクラウド上で行われていることが分かるでしょう。長期的には、より多くのAIワークロードがデバイス上で実行されるようになると予想されます。しかし今現在、<strong>本番環境の音声AIはクラウドが中心です</strong>。その理由は2つあります：
          </p>

          <ol class="list-decimal">
            <li>
              音声AIエージェントは、低遅延で確実に複雑なワークフローを実行するために、利用可能な最良のAIモデルを使用する必要があります。エンドユーザーデバイスはまだ、最高のSTT、LLM、TTSモデルを許容できる遅延で実行するのに十分な処理能力をもったCPUを持っていません。
            </li>
            <li>今ある商用音声AIエージェントの大半は、電話を介してユーザーとやり取りしています。電話の場合、エンドユーザーデバイスは存在しません — 少なくとも、あなたがコードを実行できるようなデバイスはありません！
            </li>
          </ol>

          <p>このエージェントオーケストレーションの世界に深く入り込み<sup>[6]</sup>、次のような質問に答えていきましょう：</p>

          <ol class="list-decimal">
            <li>音声AIエージェントにはどのLLMが最適か？</li>
            <li>長時間のセッション中に会話コンテキストをどのように管理するか？</li>
            <li>音声AIエージェントを既存のバックエンドシステムにどのように接続するか？<sup>[7]</sup></li>
            <li>音声AIエージェントが適切に機能しているかどうかをどう判断するか？</li>
          </ol>

        </div>

        <div class="chunk-notes">
          <div class="chapter-image">
            <img src="../images/Figure 0200.svg" class="image-hide-narrow" width="100%">
            <p class="image-caption image-hide-narrow">こんにちのほぼすべての本番環境でのアーキテクチャ</p>
          </div>
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[5] 例えば、一般的なLLMエラーや安全性の問題を検出するため。</p>
            </div>
            <div class="footnote">
              <p>[6] 詳細に説明しましょう — 編注。</p>
            </div>
            <div class="footnote">
              <p>[7] 例えば、CRM、独自のナレッジベース、コールセンターシステムなど。</p>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h1 id="core-tech">4. コア技術とベストプラクティス</h1>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h2 id="latency">4.1. レイテンシ（遅延）</h2>

          <p>音声AIエージェントの構築は、ほとんどの点で他の種類のAIエンジニアリングと類似しています。テキストベースのマルチターンAIエージェントの構築経験があれば、そのドメインで得た多くの経験がボイスにも役立ちます。
          </p>

          <p><strong>大きな違いはレイテンシです。</strong></p>

          <p>人間は通常の会話で速い応答を期待します。典型的な応答時間は500msです。長い沈黙は不自然に感じられます。</p>

          <p>音声AIエージェントを構築するなら、エンドユーザーの視点からレイテンシを正確に測定する方法を学ぶ価値があります。</p>

          <p>
            多くの場合、AIプラットフォームは実際の「音声対音声」の測定ではないレイテンシを提示しています。悪意があってしているわけではありません。プロバイダ側から見ると、レイテンシを測る簡単な方法は推論時間を測ることです。そのためプロバイダはレイテンシをそう考えがちです。しかし、このサーバー側の視点はオーディオ処理、フレーズのエンドポイント遅延、ネットワーク転送、およびOSのオーバーヘッドを考慮しません。
          </p>

          <p><strong>音声対音声のレイテンシを測るのは手動でも簡単です。</strong></p>

          <p>会話を録音し、その録音をオーディオ編集ソフトに読み込み、オーディオ波形を見て、ユーザーの発話の終了からLLMの発話の開始までを測定するだけです。</p>

          <p>本番用途の会話型ボイスアプリケーションを構築する場合、時折この方法でレイテンシの数値をさっと確認するとよいです。テスト時にネットワークのパケット損失とジッターをシミュレートするとさらによいです！</p>

          <p>
            音声から音声のレイテンシをプログラム的に測定するのは難しいです。一部のレイテンシはOSの深部で発生します。したがって、多くの可観測性ツールは最初の（オーディオ）バイト到着までの時間を測定します。これは総合的な音声対音声レイテンシの合理的な代理指標ですが、フレーズのエンドポイント変動やネットワークの往復時間など、測定していない要素が追跡手段がなければ問題になる可能性があることに注意してください。
          </p>

          <p>
            <strong>会話型AIアプリケーションを構築しているなら、音声対音声で800msのレイテンシを目標にするのが良いでしょう。</strong>以下はユーザーのマイクからクラウドに送り、戻ってくる音声対音声のラウンドトリップの内訳です。これらの数値はかなり一般的で、合計は約1秒です。こんにちのLLMで安定して800msのラウンドトリップ時間を達成するのはチャレンジですが、不可能ではありません！
          </p>

          <table class="data-table latency-breakdown">
            <thead>
              <tr>
                <th>ステージ</th>
                <th>時間（ms）</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>macOS マイク入力</td>
                <td>40</td>
              </tr>
              <tr>
                <td>opus エンコード</td>
                <td>21</td>
              </tr>
              <tr class="network-row">
                <td>ネットワークスタックと転送</td>
                <td>10</td>
              </tr>
              <tr>
                <td>パケット処理</td>
                <td>2</td>
              </tr>
              <tr>
                <td>ジッターバッファ</td>
                <td>40</td>
              </tr>
              <tr>
                <td>opus デコード</td>
                <td>1</td>
              </tr>
              <tr>
                <td>文字起こしとエンドポイント検出</td>
                <td>300</td>
              </tr>
              <tr>
                <td>LLM 初回バイト（ttfb）</td>
                <td>350</td>
              </tr>
              <tr>
                <td>文の集約</td>
                <td>20</td>
              </tr>
              <tr>
                <td>TTS 初回バイト（ttfb）</td>
                <td>120</td>
              </tr>
              <tr>
                <td>opus エンコード</td>
                <td>21</td>
              </tr>
              <tr>
                <td>パケット処理</td>
                <td>2</td>
              </tr>
              <tr class="network-row">
                <td>ネットワークスタックと転送</td>
                <td>10</td>
              </tr>
              <tr>
                <td>ジッターバッファ</td>
                <td>40</td>
              </tr>
              <tr>
                <td>opus デコード</td>
                <td>1</td>
              </tr>
              <tr>
                <td>macOS スピーカー出力</td>
                <td>15</td>
              </tr>
              <tr>
                <td>合計（ms）</td>
                <td>993</td>
              </tr>
            </tbody>
          </table>

          <p class="table-caption">音声対音声会話のラウンドトリップ — レイテンシの内訳</p>

          <p>
            我々は、すべてのモデルを同一のGPU対応クラスタ内にホスティングし、スループットではなくレイテンシ最適化を行うことで500msの音声対音声レイテンシを達成するPipecatエージェントを実証しました。このアプローチは現状では広く使われていません。モデルのホスティングは高コストです。また、オープンウェイトのLLMは、GPT-4oやGeminiのような最良の独自モデルに比べて音声AIで使われる頻度が低いです。音声AIエージェント向けのLLMについては次節を参照してください。
          </p>

          <p>ボイスユースケースではレイテンシが非常に重要なため、本ガイド全体でレイテンシが頻繁に取り上げられます。</p>
        </div>
        <div class="chunk-notes">
          <div class="image-hide-narrow">
            <img src="../images/Figure 0300.svg" width="100%">
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h2 id="llms-for-voice">4.2. ボイスユースケース向けのLLM</h2>

          <p>
            2023年3月のGPT-4のリリースは、現在の音声AI時代の幕開けとなりました。GPT-4は、柔軟なマルチターン会話を維持でき、かつ有用な作業を行うために十分に正確な最初のLLMでした。今日では、GPT-4の後継であるGPT-4oが会話型音声AIの支配的なモデルとなっています。
          </p>

          <p>他のモデルは、元のGPT-4よりもいくつかの点において同等かそれ以上の性能を発揮するようになっています：</p>

          <ul class="arrow-list">
            <li>対話型の音声会話に耐える十分低いレイテンシ。</li>
            <li>指示への良い従順性。<sup>[8]</sup></li>
            <li>信頼できるFunction Calling。<sup>[9]</sup></li>
            <li>ハルシネーションやその他の不適切な応答の発生率が低い。</li>
            <li>個性とトーン。</li>
            <li>コスト。</li>
          </ul>
        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[8] モデルに特定のことを実行させるプロンプトの出しやすさはどれほどか？</p>
            </div>
            <div class="footnote">
              <p>[9] 音声AIエージェントはFunction Callingに大きく依存する。</p>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">

          <p>しかし、現状、GPT-4oはオリジナルのGPT-4よりも優れています！特に指示への従順性、Function Calling、ハルシネーションの発生率低減において優れています。</p>

          <p>GPT-4oの主要な競合はGoogleのGemini 2.0 Flashです。Gemini 2.0 Flashは高速で、指示への従順性とFunction
            CallingにおいてGPT-4oと同等であり、価格も攻勢的です。</p>

          <p><strong>音声AIのユースケースは要求が厳しいため、一般的には利用可能な最良のモデルを使うのが妥当です。</strong>
            いずれ状況は変わり、最先端でないモデルでも音声AIのユースケースで広く採用されるのに十分になる時が来るでしょう。まだその段階ではありません。</p>


        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h3 id="latency-llm">4.2.1 LLM レイテンシ</h3>

          <p>Claude Sonnetは音声AIに非常に良い選択肢となり得ますが、推論レイテンシ（最初のトークンまでの時間）がAnthropicの優先事項ではなかった点が問題です。Claude
            Sonnetの中央値レイテンシは通常、GPT-4oやGemini Flashの2倍で、P95のばらつきもはるかに大きいです。</p>

          <table class="data-table model-comparison">
            <thead>
              <tr>
                <th>モデル</th>
                <th>中央値 TTFT (ms)</th>
                <th>P95 TTFT (ms)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>GPT-4o</td>
                <td>460</td>
                <td>580</td>
              </tr>
              <tr>
                <td>GPT-4o mini</td>
                <td>290</td>
                <td>420</td>
              </tr>
              <tr>
                <td>GPT-4.1</td>
                <td>450</td>
                <td>670</td>
              </tr>
              <tr>
                <td>Gemini 2.0 Flash</td>
                <td>380</td>
                <td>450</td>
              </tr>
              <tr>
                <td>Llama 4 Maverick (Groq)</td>
                <td>290</td>
                <td>360</td>
              </tr>
              <tr>
                <td>Claude Sonnet 3.7</td>
                <td>1,410</td>
                <td>2,140</td>
              </tr>
            </tbody>
          </table>
          <p class="table-caption">最初のトークンまでの時間（TTFT）メトリクス：OpenAI、Anthropic、Google API - 2025年5月</p>

          <p>大まかな目安として、LLMのTTFTが500ms以下であればほとんどの音声AIユースケースで十分です。GPT-4oのTTFTは通常400〜500msです。Gemini Flashも同様です。</p>

        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h3 id="cost-comparison">4.2.2 コスト比較</h3>

          <p>推論コストは頻繁かつ急速に低下しています。したがって、一般にコストはLLMを選ぶ際の重要な要因ではありません。Gemini 2.0
            Flashの新たに発表された価格はGPT-4oと比べて10倍のコスト削減を実現しています。これが音声AIの状況にどのような影響を与えるかは注視が必要です。</p>

          <table class="data-table model-comparison">
            <thead>
              <tr>
                <th>モデル</th>
                <th>3分の会話</th>
                <th>10分の会話</th>
                <th>30分の会話</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>GPT-4o</td>
                <td>$0.009</td>
                <td>$0.08</td>
                <td>$0.75</td>
              </tr>
              <tr>
                <td>Gemini 2.0 Flash</td>
                <td>$0.0004</td>
                <td>$0.004</td>
                <td>$0.03</td>
              </tr>
            </tbody>
          </table>
          <p class="table-caption">
            複数の会話が発生するセッションのコストは、継続時間とともに超線形に増加します。30分のセッションは3分のセッションより概ね100倍の費用がかかります。キャッシュ、コンテキスト要約、その他の手法で長時間セッションのコストを削減できます。
          </p>

          <p>
            セッション長に対してコストは超線形に増加する点に注意してください。セッション中にコンテキストをトリミングまたは要約しない限り、長時間のセッションではコストが問題になります。これはspeech-to-speechへのモデルで特に当てはまります（下の
            <a href="#speech-to-speech">項</a> を参照）。
          </p>

          <p>
            コンテキスト増大という数学的性質により、音声会話の1分あたりコストを特定するのは難しくなります。さらに、APIプロバイダはトークンキャッシュを提供することが増えており、これはコストを相殺（およびレイテンシを低減）できますが、異なるユースケースのコスト見積もりを複雑にします。
          </p>

          <p>OpenAIの <a href="https://community.openai.com/t/new-realtime-api-voices-and-cache-pricing/998238"
              target="_blank">OpenAI Realtime API向けの自動トークンキャッシュ</a> は特に便利です。Googleも最近、すべてのバージョン2.5モデル向けに <a
              href="https://ai.google.dev/gemini-api/docs/caching?lang=python" target="_blank">暗黙のキャッシュ（implicit
              caching）</a> と呼ばれる類似機能を導入しました。</p>


        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h3 id="open-source">4.2.3 オープンソース / オープンウェイト</h3>

          <p>MetaのLlama
            3.3および4.0のオープンウェイトモデルは、ベンチマーク上でGPT-4よりも良い性能を示します。とはいえ、商用ユースケースにおいてGPT-4oやGeminiより常に優れているわけではありません。しかし、それらを基に構築したり自社インフラで実行したりできる能力は重要です。<sup>[11]</sup>
          </p>

          <p>
            多くのプロバイダがLlamaの推論エンドポイントを提供しており、サーバーレスGPUプラットフォームはLlamaをデプロイするためのさまざまなオプションを提供します。Metaは最近、ファーストパーティの推論APIを発表し、オープンウェイトのLlamaモデルが同社の重要な戦略的焦点であることを強く示しました。
          </p>

          <p>Llamaファミリの中で興味深く有能なモデルの一つがUltravoxです。Ultravoxは<a href="https://github.com/fixie-ai/ultravox"
              target="_blank">オープンソースのネイティブ音声LLM</a>です。<a href="https://ultravox.ai/"
              target="_blank">Ultravoxの背後にある会社</a>は、商用レベルの音声対話APIも提供しています。Ultravoxは、Llama
            3.3を音声領域に拡張し、音声AIユースケース向けにベースモデルの指示従順性とFunction
            Calling性能を改善するために多数の手法を活用しています。Ultravoxは、オープンソースAIエコシステムの利点とネイティブ音声モデルの有望な可能性の両方を示す例です。</p>

          <p>我々は2025年に入ってオープンソース／オープンウェイトモデルの多くの進展を見ています。Llama 4はごく新しく、コミュニティは対話型の会話AIにおける実用的な性能をまだ評価中です。Alibabaの新しいQwen
            3モデルは優れた中規模モデルで、初期ベンチマークではLlama
            4と互角に競っています。さらに、DeepSeek、Google（Gemma）、Microsoft（Phi）から将来登場するオープンウェイトモデルも音声AIにとって良い選択肢になる可能性が高いと考えられます。</p>
        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[11] 特定のユースケース向けにLLMをファインチューニングする予定があるなら、Llama 3.3 70Bは非常に良い出発点です。ファインチューニングについては下記を参照してください。</p>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h3 id="speech-to-speech">4.2.4 音声→音声モデルはどうか？</h3>

          <p>
            音声→音声モデルは興味深く、比較的新しい発展です。音声→音声LLMはテキストではなく音声でプロンプトを与えられ、直接音声を出力できます。これにより、音声エージェントのオーケストレーションループにおける音声→テキストおよびテキスト→音声の部分が排除されます。
          </p>

          <p>音声→音声モデルの潜在的な利点は次のとおりです：</p>

          <ul class="arrow-list">
            <li>レイテンシーの低下。</li>
            <li>人間の会話のニュアンスを理解する能力の向上。</li>
            <li>より自然な音声出力。</li>
          </ul>

          <p>OpenAIとGoogleは共に音声→音声のAPIを公開しています。大規模モデルを訓練し音声AIアプリケーションを構築する多くの人々は、音声→音声モデルが音声AIの未来であると考えています。</p>

          <p>しかし、現時点の音声→音声モデルとAPIは、ほとんどの本番環境の音声AIにとってまだ十分に良いとは言えません。</p>

          <p>今ある音声→音声モデルは、確かに現状のText-to-speechモデルより自然に聞こえます。OpenAIの<a
              href="https://platform.openai.com/docs/guides/audio"
              target="_blank">gpt4o-audio-preview</a><sup>[12]</sup>モデルは、音声AIの未来のプレビューのように確かに聞こえます。</p>

        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[12] <a href="https://platform.openai.com/docs/guides/audio" target="_blank">OpenAI audio API docs</a>
              </p>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <p>ただし、音声→音声モデルはまだテキストモードのLLMほど成熟しておらず信頼性も十分とは言えません。</p>

          <ul class="arrow-list">
            <li>
              理論的にはレイテンシーは低減可能ですが、音声はテキストよりも多くのトークンを使用します。トークンコンテキストが大きくなるとLLMが処理するのに時間がかかります。実際には、音声モデルは長い対話型会話において、テキストモデルより遅くなります。<sup>[13]</sup>
            </li>
            <li>これらのモデルは理解力の向上が実際の利点であるように見えます。これは特にGemini 2.0
              Flashの音声入力で顕著です。gpt-4o-audio-previewについては、テキストモードのGPT-4oより小さくやや能力が劣るモデルであるため、現時点では状況がやや不明瞭です。</li>
            <li>
              より自然な音声出力は今でも明らかに知覚できます。しかしオーディオLLMは音声モードで奇妙な出力パターンを示すことがあり、テキストモードほど頻繁ではないものの、語の反復、時に不気味の谷に入る談話マーカー、文を完結できないことが時折あります。
            </li>
          </ul>
        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[13] 音声モデルにおけるこのレイテンシー問題は、キャッシュ、巧妙なAPI設計、モデル自体のアーキテクチャ進化の組み合わせによって明確に解決可能です。</p>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <p>
            これらの問題の中で最も大きいのは、マルチターン音声に必要な大きなコンテキストサイズです。ネイティブ音声の利点を得つつコンテキストサイズの欠点を回避する一つの方法は、各会話ターンをテキストと音声の混合として処理することです。最新のユーザーメッセージには音声を使用し、残りの会話履歴はテキストを使用します。
          </p>

          <p>OpenAIのベータ版音声→音声提供であるOpenAI Realtime
            APIは高速で音声品質は驚異的です。しかし、そのAPIの背後にあるモデルはフルのGPT-4oではなく小型のgpt-4o-audio-previewです。したがって指示従順性やファンクションコールは同等ではありません。Realtime
            APIで会話コンテキストを管理するのもやや扱いにくく、APIにはいくつかの新製品ならではの粗さがあります。<sup>[14]</sup></p>

          <p>GoogleのMultimodal Live
            APIも有望であり、進化の初期段階にある音声→音声サービスです。このAPIはGeminiモデルの近未来像を示しています：長いコンテキストウィンドウ、優れたビジョン機能、高速推論、強力な音声理解、コード実行、検索に基づくグラウンディング。OpenAI
            Realtime API同様、Multimodal Live APIは現時点ではほとんどの本番音声AIアプリケーションにとって適切な選択肢とは言えません。</p>

          <p>音声→音声APIは比較的高価である点に注意してください。我々は<a href="https://dub.sh/voice-agents-010" target="_blank">OpenAI Realtime
              APIのコスト計算機</a>を作成しており、OpenAIの自動トークンキャッシュ機能を考慮してセッション長に応じたコストのスケーリングを示しています。</p>

          <p>我々は2025年に音声→音声分野で多くの進展が見られると予想しています。しかし、本番の音声AIアプリケーションがマルチモデルのアプローチから音声→音声APIへの移行をどれほど速く進めるかはまだ不明です。</p>

        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[14] <a href="https://latent.space/p/realtime-api" target="_blank">Realtime APIに関する詳細ノート</a>を参照してください
              </p>
            </div>
          </div>
          <div class="footnote">
            <a href="https://dub.sh/voice-agents-010" target="_blank"><img src="../images/Figure 0700 Spreadsheet.png"
                width="90%"></a>
            <p class="image-caption">OpenAI Realtime API コスト計算機</p>
          </div>

        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h2 id="speech-to-text">4.3. Speech-to-text</h2>

          <p>Speech-to-text(音声→テキスト)は音声AIの「入力」段階です。Speech-to-textは一般に<i>文字起こし</i>または<i>ASR</i>（自動音声認識）とも呼ばれます。</p>

          <p>音声AIでは、非常に低い文字起こしレイテンシーと非常に低い語誤り率が必要です。残念ながら、音声モデルを低レイテンシー向けに最適化すると精度にマイナスの影響があります。</p>

          <p>
            今日、低レイテンシー向けに設計されていない非常に優れた文字起こしモデルがいくつか存在します。Whisperは多くの製品やサービスで使われているオープンソースモデルです。非常に優れていますが、通常のTTFT(ファーストトークン到達時間)が500ms以上であるため、会話型の音声AIにはめったに使われません。
          </p>
        </div>
        <div class="chunk-notes">
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h3 id="deepgram-and-gladia">4.3.1 DeepgramとGladia</h3>

          <p>多くの本番環境で利用される音声AIエージェントはSpeech-to-textに<a href="https://deepgram.com" target="_blank">Deepgram</a>または<a
              href="https://gladia.io"
              target="_blank">Gladia</a>を使用しています。Deepgramは、低レイテンシー、低語誤り率、低コストの優れた組み合わせを長年にわたり提供してきた商用のSpeech-to-textAI研究所であり、APIプラットフォームです。Gladiaは2022年設立の比較的新しい参入企業で、多言語サポートに特に強みを持ちます。
          </p>

          <p>
            DeepgramのモデルはセルフサーブのAPIとして、または顧客が自社システムで実行できるDockerコンテナとして提供されています。ほとんどの人はまずAPI経由でDeepgramのSpeech-to-textを使用し始めます。米国内のユーザではファーストトークン到達時間は通常約150msです。
          </p>

          <p>
            スケーラブルなGPUクラスタを管理することは継続的な大きなDevOps作業であるため、APIから自社インフラでモデルをホストすることに移行するのは正当な理由がない限り行うべきではありません。正当な理由には次が含まれます：
          </p>

          <ul class="arrow-list">
            <li>
              音声／文字起こしデータをプライベートに保つこと。DeepgramはBAAや機密データ処理に関する契約を提供していますが、一部の顧客は音声および文字起こしデータを完全に管理したいと考えるでしょう。米国外の顧客は自国または地域内にデータを保持する法的義務がある場合があります。（デフォルトではDeepgramの利用規約はAPI経由で送信したすべてのデータで学習することを許可しています。エンタープライズプランではこれをオプトアウトできます。）
            </li>
            <li>レイテンシーの削減。Deepgramは米国外に推論サーバーを持ちません。ヨーロッパからのTTFTは約250ms、インドからは約350msです。</li>
          </ul>

          <p>Deepgramはファインチューニングサービスを提供しており、ユースケースに比較的珍しい語彙、話し方、またはアクセントが含まれる場合に誤り率を下げるのに役立ちます。</p>

          <p>
            Gladiaは英語圏外の新しい音声AIプロジェクトで最もよく見られるSpeech-to-textプロバイダです。Gladiaはフランスに本社を置き、米国とヨーロッパに推論サーバーを持ち、100以上の言語をサポートしています。
          </p>

          <p>Gladiaはホスト型APIと自社インフラでモデルを実行するオプションを提供しています。GladiaのAPIはヨーロッパのデータ居住要件があるアプリケーションで利用できます。</p>

        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h3 id="prompting-help">4.3.2 プロンプトはLLMを助ける</h3>

          <p>文字起こしの誤りの大部分は、リアルタイムストリームで文字起こしモデルが利用できるコンテキストが非常に少ないことに起因します。</p>

          <p>
            今日のLLMは文字起こし誤りを回避するのに十分賢いです。LLMが推論を行う際には完全な会話コンテキストにアクセスできます。したがって、入力がユーザー音声の文字起こしであり、その点を考慮して推論すべきであるとLLMに伝えることができます。
          </p>

          <pre><code>You are a helpful, concise, and reliable voice assistant. Your primary goal is to understand the user's spoken requests, even if the speech-to-text transcription contains errors. Your responses will be converted to speech using a text-to-speech system. Therefore, your output must be plain, unformatted text.

When you receive a transcribed user request:
1. Silently correct for likely transcription errors. Focus on the intended meaning, not the literal text. If a word sounds like another word in the given context, infer and correct. For example, if the transcription says "buy milk two tomorrow" interpret this as "buy milk tomorrow".
2. Provide short, direct answers unless the user explicitly asks for a more detailed response. For example, if the user says "what time is it?" you should respond with "It is 2:38 AM". If the user asks "Tell me a joke", you should provide a short joke.
3. Always prioritize clarity and accuracy. Respond in plain text, without any formatting, bullet points, or extra conversational filler.
4. If you are asked a question that is time dependent, use the current date, which is February 3, 2025, to provide the most up to date information.
5. If you do not understand the user request, respond with "I'm sorry, I didn't understand that."

Your output will be directly converted to speech, so your response should be natural-sounding and appropriate for a spoken conversation.
</code></pre>
          <p class="image-caption">音声AIエージェント向けのプロンプト例の文言。</p>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h3 id="other-stt-options">4.3.3 その他のSpeech-to-textの選択肢</h3>

          <p>我々は2025年にSpeech-to-text分野で多くの新しい開発が起こると予想しています。2025年4月初旬時点で注目している新展開のいくつか：</p>

          <ul class="arrow-list">
            <li>OpenAIは<a
                href="https://openai.com/index/introducing-our-next-generation-audio-models/">最近</a>、gpt-4o-transcribeとgpt-4o-mini-transcribeという2つの新しいSpeech-to-textモデルを発表しました。
            </li>
            <li>他の評判の良い音声技術企業である<a href="https://speechmatics.com/">Speechmatics</a>と<a
                href="https://assembly.ai/">AssemblyAI</a>は、会話型音声により注力し始めており、ストリーミングAPIやより高速なTTFTのモデルを提供し始めています。</li>
            <li>NVIDIAはベンチマークで非常に良好な性能を示す<a
                href="https://developer.nvidia.com/blog/new-standard-for-speech-recognition-and-translation-from-the-nvidia-nemo-canary-model/">オープンソース音声モデル</a>を提供しています。
            </li>
            <li>推論を提供する企業である<a href="https://groq.com/">Groq</a>のホスト版Whisper Large v3
              Turboは現在中央値のTTFTが300ms未満であり、会話型音声アプリケーションの選択肢の範囲に入っています。これは我々がこのレイテンシーを達成したのを確認した最初のWhisper APIサービスです。
            </li>
          </ul>

          <p>主要なクラウドサービスはすべてSpeech-to-textのAPIを提供しています。現時点で、低レイテンシの音声AIユースケースにおいては、どれもDeepgramやGladiaほど優れてはいません。</p>

          <p>ただし、次の場合は Azure AI Speech、Amazon Transcribe、または Google Speech-to-Text を使用したいことがあるかもしれません:</p>

          <ul class="arrow-list">
            <li>既にこれらのクラウドプロバイダのいずれかと大きなコミット済み支出やデータ処理の取り決めがある場合。</li>
            <li>これらのクラウドプロバイダのいずれかに大量のスタートアップクレジットがあり、使い切りたい場合！</li>
          </ul>

        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h3 id="gemini-transcribing">4.3.4 Google Gemini を使った文字起こし</h3>

          <p>Gemini 2.0 Flash を低コストのネイティブ音声モデルとして活用する一つの方法は、会話生成と文字起こしの両方に Gemini 2.0 を使うことです。</p>

          <p>これを行うには、2つの並列な推論プロセスを実行する必要があります。</p>

          <ul class="arrow-list">
            <li>一方の推論プロセスは会話の応答を生成します。</li>
            <li>もう一方の推論プロセスはユーザーの発話を文字起こしします。</li>
            <li>各音声入力は単一のターンのみに使用されます。完全な会話コンテキストは常に最新のユーザー発話の音声と、過去の全ての入力および出力のテキスト文字起こしです。</li>
            <li>これにより両方の利点が得られます: 現在のユーザー発話に対するネイティブな音声理解と、全体コンテキストのトークン数削減。<sup>[15]</sup></li>
          </ul>
        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[15]
                音声をテキストに置き換えるとトークン数は約10倍少なくなります。10分間の会話では、処理される総トークン数、したがって入力トークンのコストは約100倍削減されます。（会話履歴は各ターンで累積するため。）
              </p>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">

          <p>ここでは、これらの並列推論プロセスを Pipecat パイプラインとして実装するためのコードを示します。</p>

          <pre class="language-python"><code>
  pipeline = Pipeline( 
    [   
        transport.input(), 
        audio_collector,
        context_aggregator.user(),
        ParallelPipeline( 
            [ # transcribe
                input_transcription_context_filter,
                input_transcription_llm,
                transcription_frames_emitter,
            ],
            [ # conversation inference
                conversation_llm,
            ],
        ),
        tts,
        transport.output(),
        context_aggregator.assistant(),
        context_text_audio_fixup, 
    ] 
  )            
            </code></pre>

          <p>論理は次のとおりです。</p>

          <ol class="list-decimal">
            <li>会話用LLMは会話履歴をテキストとして受け取り、さらに各新しいターンのユーザー発話をネイティブ音声として受け取り、会話応答を出力します。</li>
            <li>文字起こし用LLMは同じ入力を受け取りますが、最新のユーザー発話の逐語的な文字起こしを出力します。</li>
            <li>各会話の最後に、ユーザーの音声コンテキスト項目はその音声の文字起こしに置き換えられます。</li>
          </ol>

          <p>Gemini のトークンあたりコストは非常に低いため、この方法は実際には文字起こしに Deepgram を使うより安価になることがあります。</p>

          <p>ここで重要なのは、Gemini 2.0 Flash
            を音声→音声モデルとして使用しているわけではなく、その音声理解能力を利用しているという点です。モデルをプロンプトして、会話モードと文字起こしモードという2つの異なる「モード」で動作させています。</p>

          <p>
            このようなLLMの使い方は、最先端のLLMアーキテクチャの持つ可能性を示します。このアプローチは新しくまだ実験的ですが、初期のテストでは他のどの手法よりも優れた会話理解とより正確な文字起こしができる可能性が示唆されています。ただし欠点もあります。文字起こしのレイテンシは専用のSpeech-to-textモデルほど良くありません。2つの推論プロセスを実行しコンテキスト要素を入れ替える複雑さは相当なものです。専用の文字起こしモデルでは問題とならないプロンプトインジェクションやコンテキスト追従エラーに対して脆弱になります。
          </p>

          <p>ここに文字起こし用のシステム指示（プロンプト）を示します。</p>

          <pre><code>
You are an audio transcriber. You are receiving audio from a user. Your job is to transcribe the input audio to text exactly as it was said by the user.

You will receive the full conversation history before the audio input, to help with context. Use the full history only to help improve the accuracy of your transcription.

Rules:
- Respond with an exact transcription of the audio input.
- Do not include any text other than the transcription.
- Do not explain or add to your response.
- Transcribe the audio input simply and precisely.
- If the audio is not clear, emit the special string "".
- No response other than exact transcription, or "", is allowed.
            </code></pre>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h2 id="text-to-speech">4.4. Text-to-speech</h2>

          <p>Text-to-speechは、音声→音声処理ループの出力段階です。</p>

          <p>音声AI開発者は次の基準で音声モデル／サービスを選びます:</p>

          <ul class="arrow-list">
            <li>音声の自然さ（総合的な品質）<sup>[16]</sup></li>
            <li>レイテンシ<sup>[17]</sup></li>
            <li>コスト</li>
            <li>対応言語</li>
            <li>単語レベルのタイムスタンプ対応</li>
            <li>声、アクセント、発音のカスタマイズ能力</li>
          </ul>
        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[16] 発音、イントネーション、話速、強勢、リズム、感情的価数。</p>
            </div>
            <div class="footnote">
              <p>[17] 最初の音声バイトまでの時間。</p>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">

          <p>音声の選択肢は2024年に大幅に増えました。新しいスタートアップが登場し、最高品質の音声は大幅に向上しました。すべての提供者がレイテンシを改善しました。</p>

          <p>Speech-to-textの場合と同様に、主要なクラウドプロバイダはすべてText-to-speech製品を持っています。<sup>[18]</sup>
            しかし現在はスタートアップのモデルの方が優れているため、多くの音声AI開発者はそれらを使っていません。</p>
        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[18] Azure AI Speech、Amazon Polly、Google Cloud Text-to-Speech。</p>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">

          <p>リアルタイム会話型ボイスモデルで最も勢いのあるラボは（アルファベット順）:</p>

          <ul class="arrow-list">
            <li>Cartesia – 独自の状態空間モデルアーキテクチャを使用して、高品質と低レイテンシを両立しています。</li>
            <li>Deepgram – レイテンシと低コストを優先しています。</li>
            <li>ElevenLabs – 感情的および文脈的なリアリズムを重視しています。</li>
            <li>Rime – 会話音声のみで訓練されたカスタマイズ可能なTTSモデルを提供しています。</li>
          </ul>

          <p>これら4社はいずれも強力なモデル、エンジニアリングチーム、安定かつ高性能なAPIを持っています。Cartesia、Deepgram、Rime のモデルは自社インフラにデプロイできます。</p>

          <table class="data-table voice-model-breakdown">
            <thead>
              <tr>
                <th> </th>
                <th>1分あたりのコスト（概算）</th>
                <th>中央値 TTFB (ms)</th>
                <th>P95 TTFB (ms)</th>
                <th>平均プレスピーチ ms</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Cartesia</td>
                <td>$0.02</td>
                <td>190</td>
                <td>260</td>
                <td>160</td>
              </tr>
              <tr>
                <td>Deepgram</td>
                <td>$0.008</td>
                <td>150</td>
                <td>320</td>
                <td>260</td>
              </tr>
              <tr>
                <td>ElevenLabs Turbo v2</td>
                <td>$0.08</td>
                <td>300</td>
                <td>510</td>
                <td>160</td>
              </tr>
              <tr>
                <td>ElevenLabs Flash v2</td>
                <td>$0.04</td>
                <td>170</td>
                <td>190</td>
                <td>100</td>
              </tr>
              <tr>
                <td>Rime</td>
                <td>$0.024</td>
                <td>340</td>
                <td>980</td>
                <td>160</td>
              </tr>
            </tbody>
          </table>
          <p class="table-caption">1分あたりの概算コストおよびTime to first byteまでの時間の指標 – 2025年2月。コストは利用量や使用する機能に依存する点に注意してください。avg
            pre-speechは、最初の発話フレームの前にオーディオストリーム内で発生する平均的な初期無音間隔です。</p>

          <p>
            Speech-to-textと同様に、非英語言語のサポートには音声モデルによって大きな差があります。非英語のユースケース向けに音声AIを構築する場合は、より広範なテストが必要になる可能性が高いです。様々なサービスや複数の音声をテストして、満足できるソリューションを見つけてください。
          </p>

          <p>すべての音声モデルは時折単語を誤発音するし、固有名詞や珍しい語の発音を必ずしも知っているわけではありません。</p>

          <p>
            一部のサービスは発音を調整する機能を提供しています。これは、出力テキストに特定の固有名詞が含まれることが事前に分かっている場合に便利です。もし音声サービスが音素ベースの調整をサポートしていない場合、LLMに特定の単語の「音のような」綴りを出力させるようプロンプトすることもできます。例えば、NVIDIAの代わりに
            in-vidia と発音させたり。</p>

          <pre><code class="nobreak">
              Replace "NVIDIA" with "in vidia" and replace <br>
              "GPU" with "gee pee you" in your responses.
            </code></pre>
          <p class="image-caption">LLMのテキスト出力を使って発音を誘導するためのプロンプト例の文言</p>




          <p>
            会話型の音声では、ユーザーが聞いたテキストを追跡できることが、正確な会話コンテキストを維持するうえで重要です。これは、モデルが音声に加えて単語レベルのタイムスタンプメタデータを生成し、タイムスタンプデータが元の入力テキストへ逆変換可能であることを要求します。これは音声モデルにとって比較的新しい機能です。上の表にあるモデルのうちElevenLabs
            Flashを除くすべてが単語レベルのタイムスタンプをサポートしています。</p>

          <pre><code>
{
  "type": "timestamps",
  "context_id": "test-01",
  "status_code": 206,
  "done": false,
  "word_timestamps": {
    "words": ["What's", "the", "capital", "of", "France?"],
    "start": [0.02, 0.3, 0.48, 0.6, 0.8],
    "end": [0.3, 0.36, 0.6, 0.8, 1]
  }
}
                    </code></pre>

          <p class="image-caption">Cartesia APIからの単語レベルタイムスタンプ。</p>

          <p>
            さらに、リアルタイムストリーミングAPIが利用できればとても便利です。会話型音声アプリケーションでは同時に複数の音声推論を行うことがよくあります。音声エージェントには、進行中の推論を中断でき、各推論リクエストを出力ストリームに紐づけるような実装が必要になります。音声モデルプロバイダのストリーミングAPIは比較的新しく、まだ進化途上です。現時点では、CartesiaとRimeが最も成熟したストリーミングサポートをPipecatで使える形で提供しています。
          </p>

          <p>音声モデルの進歩は2025年も続くと予想されます。</p>

          <ul class="arrow-list">
            <li>上記に記載された企業のいくつかは、年の前半に新モデルを出すことを示唆しています。</li>
            <li>OpenAIは<a href="https://openai.com/index/introducing-our-next-generation-audio-models/"
                target="_blank">最近、新しいテキスト読み上げモデル</a> gpt-4o-mini-tts
              をリリースしました。このモデルは完全に制御可能であるため、音声モデルに対して「何を言うか」だけでなく「どのように話すか」を指示することが可能になります。gpt-4o-mini-ttsの制御を試すには<a
                href="https://openai.fm" target="_blank">openai.fm</a>で実験できます。</li>
            <li><a href="https://groq.com/" target="_blank">Groq </a>と<a href="https://play.ai/"
                target="_blank">PlayAI</a>は最近<a href="https://groq.com/build-fast-with-text-to-speech/"
                target="_blank">提携を発表しました</a>。Groqは高速な推論で知られ、PlayAIは30以上の言語をサポートする低レイテンシの音声モデルを提供します。</li>
          </ul>
        </div>

        <div class="chunk-notes">



          <div class="chunk-footnotes">
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h2 id="audio-processing">4.5. オーディオ処理</h2>

          <p>
            優れた音声AIプラットフォームやライブラリは、オーディオ取り込みと処理の複雑さをほとんど隠してくれます。しかし、複雑な音声エージェントを構築すると、いずれオーディオ処理のバグやコーナーケースに直面します。<sup>[19]</sup>
            したがって、オーディオ入力パイプラインをざっくり理解しておくにこしたことはないです。</p>
        </div>

        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[19] …これはソフトウェアのすべての事柄に一般化され、そしておそらく人生のほとんどの事柄にも当てはまります。</p>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h3 id="avoiding-spurious-interruptions">4.5.1 マイクと自動利得制御</h3>

          <p>
            現代のマイクは、高度なハードウェアと大量の低レイヤーソフトウェアが組み合わさった非常に洗練されたデバイスです。これは素晴らしいことで、モバイル機器、ノートパソコン、Bluetoothイヤーピースなどに内蔵された小型マイクから優れたオーディオを得られます。
          </p>

          <p>
            しかし時には、この低レイヤーのソフトウェアが望んだ通りに動作しないことがあります。特にBluetoothデバイスは音声入力に数百ミリ秒のレイテンシを追加することがあります。これは音声AI開発者にとって制御外であることが多いですが、特定のユーザーが使用しているOSや入力デバイスによってレイテンシが大きく異なる可能性がある点は把握しておくべきです。
          </p>

          <div class="chunk-image-inline">
            <img src="../images/Figure 1600.jpg" alt="Bluetooth is problematic? Always has been."
              class="microphone-image" width="98%">
          </div>
          <p>
            ほとんどのオーディオキャプチャパイプラインは入力信号に対して何らかの自動利得制御（AGC）を適用します。これも通常は望ましい挙動で、ユーザーのマイクからの距離などを補正してくれます。一部の自動利得制御は無効にできることがありますが、消費者向けデバイスでは完全に無効にできない場合が多いです。
          </p>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h3 id="echo-cancellation">4.5.2 エコーキャンセレーション</h3>

          <p>
            ユーザーが電話を耳に当てていたりヘッドフォンを着用している場合、ローカルマイクとスピーカー間のフィードバックを心配する必要はありません。しかし、スピーカーフォンで会話している場合やヘッドフォンなしのノートパソコンを使っている場合は、優れたエコーキャンセレーションが極めて重要です。
          </p>

          <p>
            エコーキャンセレーションはレイテンシに非常に敏感なため、エコーキャンセレーションはデバイス上で実行する必要があります。今日では電話システム、ウェブブラウザ、WebRTCを使ったネイティブアプリケーションのSDKに組み込まれています。<sup>[20]</sup>
          </p>

          <p>
            したがって、音声AI、WebRTC、または電話向けSDKを使用している場合、ほとんどの実世界シナリオで「そのまま動作する」エコーキャンセレーションが利用できるはずです。自前で音声キャプチャパイプラインを構築している場合は、エコーキャンセレーションロジックを統合する方法を見つける必要があります。例えば、WebSocketベースのReact
            Nativeアプリを構築している場合、デフォルトではエコーキャンセレーションはありません。<sup>[21]</sup></p>
        </div>

        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[20]
                Firefoxのエコーキャンセレーションはあまり優れていない点に注意してください。音声AI開発者はChromeとSafariを主なプラットフォームとして構築し、時間が許せばFirefoxでテストすることを推奨します。
              </p>
            </div>
            <div class="footnote">
              <p>[21] つい最近、ある人のReact Nativeアプリのオーディオ問題のデバッグを手伝いました。根本原因は、音声AIやWebRTC
                SDKを使っていなかったためエコーキャンセレーションを実装する必要があることに気づいていなかったことでした。</p>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h3 id="noise-suppression">4.5.3 ノイズ抑制、音声、音楽</h3>

          <p>
            電話システムやWebRTCのオーディオ取り込みパイプラインは、ほとんど常に「音声モード」をデフォルトにします。音声は音楽よりもはるかに圧縮可能で、狭帯域信号に対するノイズ低減やエコーキャンセレーションアルゴリズムは実装が容易です。
          </p>

          <p>
            多くの電話プラットフォームは8kHz音声しかサポートしていません。これは現代の基準では明らかに低品質です。この制限があるシステムを経由している場合、対処のしようがありません。ユーザーが品質の低さに気付くかどうかは場合によります
            — ほとんどの人は電話通話の音質に対して期待値が低いです。</p>

          <p>WebRTCは非常に高品質なオーディオをサポートします。<sup>[22]</sup> WebRTCのデフォルト設定は通常、48kHzサンプルレート、モノラル、32
            kbpsのOpusエンコード、および適度なノイズ抑制アルゴリズムです。これらの設定は音声向けに最適化されており、幅広いデバイスや環境で機能するため、音声AIには一般的に適切な選択です。</p>

          <p>これらの設定では音楽は良い音にはなりません！</p>

          <p>WebRTC接続で音楽を送信する必要がある場合、次のようなことを行うとよいでしょう：</p>

          <ul class="arrow-list">
            <li>エコーキャンセレーションをオフにする（ユーザーはヘッドフォンを着用する必要があります）。</li>
            <li>ノイズ抑制をオフにする。</li>
            <li>必要に応じてステレオを有効にする。</li>
            <li>Opusのエンコードビットレートを上げる（モノラルなら64 kbps、ステレオなら96 kbpsまたは128 kbpsが良い目安です）。</li>
          </ul>
        </div>

        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[22] 高品質オーディオのユースケースの例：</p>
              <ul class="arrow-list">
                <li>LLM教師による音楽レッスン。</li>
                <li>背景音や音楽を含むポッドキャストの録音。</li>
                <li>対話的に生成されるAI音楽。</li>
              </ul>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h3 id="encoding">4.5.4 エンコーディング</h3>

          <p>エンコーディングは、音声データをネットワーク接続で送信するためにどのようにフォーマットするかの総称です。<sup>[23]</sup></p>
        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[23]（またはファイルに保存する場合。）</p>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="" chunk-content"="">

          <p>リアルタイム通信で一般的なエンコーディングには次のものがあります：</p>

          <ul class="arrow-list">
            <li>16ビットPCM形式の非圧縮オーディオ。</li>
            <li>Opus — WebRTCおよび一部の電話システム。</li>
            <li>G.711 — 幅広いサポートを持つ標準的な電話用コーデック。</li>
          </ul>

          <table class="data-table">
            <tbody>
              <tr>
                <th>コーデック</th>
                <th>ビットレート</th>
                <th>品質</th>
                <th>ユースケース</th>
              </tr>
              <tr>
                <td>16ビットPCM</td>
                <td>384 kbps（モノラル 24 kHz）</td>
                <td>非常に高い（ほぼ無損失）</td>
                <td>音声録音、組み込みシステム、単純なデコードが重要な環境</td>
              </tr>
              <tr>
                <td>Opus 32 kbps</td>
                <td>32 kbps</td>
                <td>良好（音声に最適化されたサイコアコースティック圧縮）</td>
                <td>ビデオ通話、低帯域幅ストリーミング、ポッドキャスティング</td>
              </tr>
              <tr>
                <td>Opus 96 kbps</td>
                <td>96 kbps</td>
                <td>非常に良い〜優秀（サイコアコースティック圧縮）</td>
                <td>ストリーミング、音楽、音声アーカイブ</td>
              </tr>
              <tr>
                <td>G.711（8 kHz）</td>
                <td>64 kbps</td>
                <td>低品質（帯域幅制限、音声中心）</td>
                <td>レガシーなVoIPシステム、電話、ファクス伝送、音声メッセージング</td>
              </tr>
            </tbody>
          </table>
          <p class="image-caption">音声AIで最も頻繁に使用されるオーディオコーデック</p>

          <p>
            これら三つの選択肢の中では、Opusが圧倒的に最良です。Opusはウェブブラウザに組み込まれており、低遅延コーデックとしてゼロから設計されていて非常に効率的です。幅広いビットレートで良好に動作し、音声と高忠実度の両方のユースケースをサポートします。
          </p>

          <p>
            16ビットPCMは「生のオーディオ」です。サンプリングレートとデータ型が正しく指定されていれば、PCMのオーディオフレームをソフトウェアのサウンドチャネルに直接送信できます。ただし、この非圧縮オーディオは一般的にインターネット接続で送信したいものではないことに注意してください。24
            kHzのPCMはビットレートが384 kbpsです。これは多くの実際のエンドユーザーデバイスからの接続ではリアルタイムでバイトを配信するのが難しいほど大きなビットレートです。</p>
        </div>

        <div class="chunk-notes">
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h3 id="server-side-noise">4.5.5 サーバーサイドノイズ処理と話者分離</h3>

          <p>
            音声認識（Speech-to-text）や音声活動検出モデルは、通常、一般的な環境ノイズ（街の音、犬の鳴き声、マイク近くの大きなファン、キーボードのクリックなど）を無視できます。したがって、多くの人間同士のユースケースで重要な従来の「ノイズ除去」アルゴリズムは、音声AIにとってそこまで重要ではありません。
          </p>

          <p>しかし、音声AIに特に有用な音声処理が一つあります：主要話者の分離です。主要話者の分離は背景の会話を抑制します。これにより文字起こしの精度が大幅に向上する可能性があります。</p>

          <p>
            空港のような環境から音声エージェントに話しかけようとする状況を想像してください。携帯のマイクはゲート案内や通行人の会話など、多くの背景音を拾いやすいです。LLMが見るテキストの文字起こしにそうした背景音が入って欲しくありません！
          </p>

          <p>
            または、リビングでテレビやラジオが背景で流れているユーザーを想像してください。人間は低音量の背景会話を比較的うまくフィルタリングできるため、カスタマーサポートに電話する前にテレビやラジオを消すことを思いつかない場合があります。
          </p>

          <p>独自の音声AIパイプラインで使用できる最良の話者分離モデルは、<a href="https://krisp.ai"
              target="_blank">Krisp</a>が提供しています。ライセンスは主にエンタープライズユーザーを対象としており、安価ではありません。しかし、大規模な商用ユースケースにおいては、コストにみあう音声エージェントの性能向上が見込めます。
          </p>

          <p>OpenAIは最近、Realtime APIの一部として新しいノイズ低減機能を出荷しました。参照ドキュメントは<a
              href="https://platform.openai.com/docs/guides/realtime-transcription#realtime-transcription-sessions"
              target="_blank">こちら</a>です。</p>

          <pre class="language-python"><code>
  pipeline = Pipeline(
    [
      transport.input(),
      krisp_filter,
      vad_turn_detector,
      stt,
      context_aggregator.user(), 
      llm, 
      tts, 
      transport.output(), 
      context_aggregator.assistant(),
    ]
  )
            </code></pre>
          <p class="image-caption">Krisp処理要素を含むPipecatパイプライン</p>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h3 id="voice-activity-detection">4.5.6 音声活動検出</h3>

          <p>音声活動検出（VAD）のステージは、ほとんどすべての音声AIパイプラインの一部です。VADはオーディオセグメントを「話し声」と「無音（話していない）」に分類します。VADについては下の<a
              href="#turn-detection">ターン検出</a>セクションで詳しく説明します。</p>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h2 id="network-transport">4.6. ネットワーク輸送</h2>

        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h3 id="websockets-webrtc">4.6.1 WebSocketsとWebRTC</h3>

          <p>WebSocketsとWebRTCはいずれもオーディオストリーミングにAIサービスで使用されます。</p>

          <p>WebSocketsはサーバー間ユースケースに最適です。遅延が主要な懸念でないユースケースでも問題なく、プロトタイピングや一般的なハッキングに適しています。</p>

          <p>WebSocketsは本番環境におけるクライアント—サーバー間のリアルタイムメディア接続で使用すべきではありません。</p>

          <p>ブラウザまたはネイティブモバイルアプリを構築していて、会話遅延（対話的レイテンシ）がアプリケーションにとって重要である場合は、アプリから音声を送受信するためにWebRTC接続を使用するべきです。</p>

          <p>エンドユーザーデバイスへのリアルタイムメディア配信におけるWebSocketsの主な問題点は次のとおりです：</p>

          <ul class="arrow-list">
            <li>WebSockets は TCP 上に構築されているため、オーディオストリームはHOLブロッキングの影響を受けます。</li>
            <li>WebRTC に使用される Opus オーディオコーデックは、WebRTC の帯域推定やパケットペーシング（輻輳制御）ロジックと密に連携しており、WebSocket
              接続だとレイテンシを蓄積してしまうようなネットワーク挙動に対しても WebRTC のオーディオストリームは対応できます。</li>
            <li>Opus
              オーディオコーデックは非常に優れた前方誤り訂正を備えており、比較的高いパケット損失に対してもオーディオストリームを堅牢にします。（ただし、これはネットワークトランスポートが遅延到着パケットを破棄でき、ヘッドオブラインブロッキングを行わない場合のみ有効です。）
            </li>
            <li>WebRTC のオーディオは自動的にタイムスタンプが付与されるため、再生や途切れ検知のロジックは自明になります。</li>
            <li>WebRTC ではパフォーマンスおよびメディア品質の統計データを取得できます。優れた WebRTC プラットフォームは詳細なダッシュボードと分析を提供します。このレベルの可観測性は、WebSockets
              に対しては非常に困難か不可能に近い場合があります。</li>
            <li>WebSocket の再接続ロジsックは堅牢に実装するのがかなり難しいです。ping/ack フレームワークを構築する必要があるか、WebSocket
              ライブラリが提供するフレームワークを十分にテストして理解する必要があります。TCP のタイムアウトや接続イベントはプラットフォームによって挙動が異なります。</li>
            <li>最後に、今日の優れた WebRTC 実装には非常に優れたエコーキャンセル、ノイズ抑制、および自動利得制御が備わっています。</li>
          </ul>

          <p>WebRTC は 2 つの方法で使用できます。</p>
          <ol>
            <li>クラウド内の WebRTC サーバーを経由するルーティング。</li>
            <li>クライアントデバイスと音声AI プロセスの間で直接接続を確立すること。</li>
          </ol>

          <p>クラウドサーバーを経由するルーティングは、多くの現実的なユースケースでより良いパフォーマンスを発揮します（下記の <a href="#network-routing">network routing</a>
            を参照）。クラウドインフラストラクチャは、直接接続では容易に、あるいは大規模にサポートしにくい多数の機能（マルチ参加者セッション、電話システムとの統合、録音など）を可能にします。</p>

          <p>しかし「サーバーレス」WebRTC は多くの音声AIのユースケースに適しています。Pipecat は <a
              href="https://docs.pipecat.ai/server/services/transport/small-webrtc"
              target="_blank">SmallWebRTCTransport</a> クラスを通じてサーバーレス WebRTC をサポートしています。また、HuggingFace の <a
              href="https://fastrtc.org/" target="_blank">FastRTC</a> のようなフレームワークは、このネットワーキングパターンを中心に構築されています。</p>

        </div>
        <div class="chunk-notes">
          <div class="chapter-image-positioned">
            <img src="../images/Figure 1900.svg" alt="WebSocket vs WebRTC diagram" class="network-image" width="250">
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h3 id="http">4.6.2 HTTP</h3>

          <p>HTTP は音声AI にとってもなお有用かつ重要です！HTTP はインターネット上のサービス相互接続の共通言語です。REST API は HTTP です。Webhook も HTTP です。</p>

          <p>テキスト指向の推論は HTTP を通じて行われるため、音声AI パイプラインは通常、会話ループの LLM 部分のために HTTP API を呼び出します。</p>

          <p>音声エージェントは外部サービスや内部 API と統合する際にも HTTP を使用します。便利な手法の一つは、LLM のFunction Callingを HTTP
            エンドポイントにプロキシすることです。これにより、音声AI エージェントの実装やDevOpsをFunction Callingの実装から切り離せます。</p>

          <p>マルチモーダル AI アプリケーションは、HTTP と WebRTC
            の両方のコードパスを実装したくなることがよくあります。テキストモードとボイスモードの両方をサポートするチャットアプリを想像してください。会話状態はどちらの接続経路からもアクセスできる必要があり、これはクライアント側とサーバー側両方のコード（例えば
            Kubernetes ポッドや Docker コンテナのアーキテクチャなど）に影響を及ぼします。</p>

          <p>HTTP の 2 つの欠点は、レイテンシと長寿命の双方向接続を実装する難しさです。</p>

          <ul class="arrow-list">
            <li>暗号化された HTTP 接続のセットアップには複数のネットワークリクエスト往復が必要です。メディア接続のセットアップ時間を 30ms
              より短くするのは相当難しく、最適化されたサーバーでも実際の最初のバイト送信までの時間は 100ms 前後になることが多いです。</li>
            <li>長時間維持される双方向の HTTP 接続は管理が難しいため、通常は WebSockets を使用した方が良いです。</li>
            <li>HTTP は TCP ベースのプロトコルなので、WebSockets に影響するのと同じヘッドオブラインブロッキングの問題が HTTP にも発生します。</li>
            <li>HTTP 上で生のバイナリデータを送ることは一般的でないため、多くの API はバイナリを base64 エンコードすることを選び、これによりメディアストリームのビットレートが増加します。</li>
          </ul>

          <p>ここで QUIC の話になります …</p>

        </div>
        <div class="chunk-notes">
          <div class="chapter-image-positioned image-position-adjust" style="--position-offset: 100px;">
            <img src="../images/Figure 2200.svg" alt="HTTP API diagram" class="network-image" width="250">
            <p class="image-caption">HTTP と WebRTC の両方をネットワーク通信に使用する音声AI エージェント。</p>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h3 id="quic-moq">4.6.3 QUIC と MoQ</h3>

          <p>QUIC は最新バージョンの HTTP（HTTP/3）のトランスポート層として、さらに柔軟に他のインターネット規模のユースケースもサポートするよう設計された新しいネットワークプロトコルです。</p>

          <p>QUIC は UDP ベースのプロトコルであり、HTTP に関連する上記のすべての問題に対処します。QUIC により接続時間の短縮、双方向ストリーム、ヘッドオブラインブロッキングの解消が得られます。Google や
            Facebook は QUIC を着実に展開しており、現在では一部の HTTP リクエストがインターネット上を TCP ではなく UDP パケットとして通過することがあります。<sup>[24]</sup></p>
        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[24] 長年インターネット上で構築してきた人にとっては少し 🤯 なことです。HTTP は常に TCP ベースのプロトコルだと考えられてきました！</p>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">

          <p>QUIC はインターネット上のメディアストリーミングの将来において大きな役割を果たすでしょう。リアルタイムメディアストリーミング向けの QUIC ベースプロトコルへの移行には時間がかかります。QUIC
            ベースの音声エージェントを構築する上での障壁の一つは、Safari がまだ WebSockets の QUIC ベース進化版である <a
              href="https://w3c.github.io/webtransport/" target="_blank">WebTransport</a> をサポートしていないことです。</p>

          <p>IETF の Media over QUIC ワーキンググループ<sup>[25]</sup>
            は「メディアの取り込みと配信のためのシンプルな低遅延メディア配信ソリューション」を開発することを目的としています。すべての標準化活動に共通するように、重要なユースケースの最大の幅を最も単純な構成要素でサポートする方法を詰めるのは容易ではありません。人々はオンデマンドビデオストリーミング、大規模なビデオ放送、ライブビデオストリーミング、多数参加者の低遅延セッション、低遅延の1:1
            セッションなどに QUIC を使うことに期待しています。</p>
        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[25] <a href="https://datatracker.ietf.org/group/moq/about/" target="_blank">IETF Media Over QUIC
                  working group</a></p>
            </div>
          </div>
        </div>
      </div>
      <div class="chunk-row">
        <div class="chunk-content">

          <p>リアルタイムの音声AI ユースケースは、MoQ 標準の開発に影響を与えるのにちょうど良いタイミングで成長しています。</p>

        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h3 id="network-routing">4.6.4 ネットワークルーティング</h3>

          <p>どのようなプロトコル基盤であっても長距離のネットワーク接続におけるレイテンシとリアルタイムメディアの信頼性は課題です。</p>

          <p><strong>リアルタイムメディア配信では、サーバーをユーザーにできるだけ近づけるべきです。</strong></p>

          <p>例えば、英国のユーザーから北カリフォルニアにある AWS us-west-1 にホストされたサーバーへの往復パケット時間は通常約 140 ミリ秒になります。比較として、同じユーザーから AWS eu-west-2
            への RTT は一般的に 15 ミリ秒以下です。</p>
          <div class="chapter-image">
            <img src="../images/Figure 2300.svg" alt="Edge routing diagram" class="network-image" width="700">
            <p class="image-caption">英国のユーザーから AWS us-west-1 への RTT は AWS eu-west-2 への RTT より約 100ms 大きい</p>
          </div>
          <p>それは 100 ミリ秒以上の差です — 音声間レイテンシ目標が 1,000 ミリ秒である場合、あなたのレイテンシ「予算」の 10% に相当します。</p>

          <p><strong>エッジルーティング</strong></p>

          <p>すべてのユーザーの近くにサーバーを展開できるとは限りません。</p>

          <p>世界中のユーザーすべてに対して 15ms RTT を達成するには、少なくとも 40 のグローバルデータセンターにデプロイする必要があります。それは大きなデブオプスの仕事です。しかも GPU
            を必要とするワークロードを実行していたり、そもそもグローバルに展開されていないサービスに依存している場合は、不可能かもしれません。</p>

          <p>光速をごまかすことはできません。<sup>[26]</sup> しかし、経路の変動性や輻輳を避けることはできます。</p>
        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[26] 古代のネットワークエンジニアの知恵 – 編者注。</p>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <p>重要なのは、パブリックインターネット経路をできるだけ短く保つことです。ユーザーを近くのエッジサーバーに接続し、そこからプライベートルートを使用します。</p>

          <p>このエッジルーティングは中央値のパケット RTT を低減します。英国 → 北カリフォルニアのプライベートバックボーン経由ルートの往復時間はおそらく約 100 ミリ秒です。100 ms（長距離プライベートルート） +
            15 ms（パブリックインターネット上の最初のホップ） = 115 ms。このプライベートルートの中央値 RTT はパブリックルートの中央値 RTT より 25ms 優れています。</p>
          <div class="chapter-image">
            <img src="../images/Figure 2400.svg" alt="Edge routing diagram" class="network-image" width="700">
            <p class="image-caption">英国から AWS us-west-1 へのエッジルート。パブリックネットワーク上の最初のホップは依然として 15ms の RTT
              を持ちます。しかし、プライベートネットワーク経由で北カリフォルニアへの長距離ルートは 100ms の RTT を持ちます。合計 RTT の 115ms は、英国から us-west-1 へのパブリックルートより
              25ms 速くなっています。また、パケット損失やジッタが少なく、変動もかなり小さくなります。</p>
          </div>
          <p>中央値 RTT の改善よりさらに重要なのは、配信の信頼性向上とジッタの低減です。<sup>[27]</sup> プライベートルートの P95 RTT はパブリックルートの P95
            よりも大幅に低くなります。<sup>[28]</sup></p>

          <p>
            これは、長距離のパブリックルート経由のリアルタイムメディア接続が、プライベートルートを使用する接続よりも測定上遅延が大きくなることを意味します。各オーディオパケットをできるだけ早く配信しようとしている一方で、パケットを順序どおり再生する必要があることを思い出してください。1
            つの遅延したパケットがあると、ジッタバッファを拡張して他の受信済みパケットを遅延パケットが到着するまで保持する必要が生じます。（あるいは、到着が長すぎると判断して空白を高度な数学で埋めるか、あるいは破綻したオーディオサンプルで埋めることになります。）
          </p>

        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[27] ジッタはパケットが経路を通過するのにかかる時間の変動性のことです。</p>
            </div>
            <div class="footnote">
              <p>[28] P95 はある指標の 95 パーセンタイル測定値です。P50 は中央値（50 パーセンタイル）です。広義には、P50 を平均的なケース、P95
                を「典型的な最悪ケース」に近い感覚を捉えるものと考えます。</p>
            </div>
          </div>
        </div>
      </div>
      <div class="chunk-row">
        <div class="chunk-content">

          <div class="chapter-image">
            <img src="../images/Figure 2500 Figure 4.w.svg" alt="Jitter buffer diagram" class="network-image"
              width="80%">
            <p class="image-caption">ジッタバッファ —
              大きなジッタバッファは音声および映像における知覚される遅延の増加に直結します。ジッタバッファをできるだけ小さく保つことは、優れたユーザー体験に大きく寄与します。</p>
          </div>
          <p>優れた WebRTC インフラ提供者はエッジルーティングを提供します。どこにサーバークラスターを持っているかを示し、プライベートルートのパフォーマンスを示すメトリクスを提供できるはずです。</p>

        </div>

        <div class="chunk-notes">
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h2 id="turn-detection">4.7. ターン検出</h2>

          <p><em>ターン検出</em> は、ユーザーが話し終えて LLM の応答を期待しているかどうかを判定することを意味します。</p>

          <p>学術文献では、この問題のさまざまな側面が <em>フレーズ検出、音声セグメンテーション、エンドポインティング</em>
            と呼ばれています。（この問題について学術文献が存在するという事実は、これは自明ではない問題であることを示す手がかりです。）</p>

          <p>私たち（人間）は誰かと話すときに毎回ターン検出を行います。しかし、いつも正しく行うわけではありません！<sup>[29]</sup></p>

          <p>したがって、ターン検出は難しい問題であり、完全な解決策は存在しません。ここでは一般的に使われているさまざまなアプローチについて説明します。</p>
        </div>

        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[29] 特に視覚的手がかりがない音声通話ではそうです。</p>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h3 id="voice-activity-detection-4-7">4.7.1 音声活動検出（VAD）</h3>

          <p>現在、音声AI エージェントのターン検出で最も一般的な方法は、長いポーズ（無音）がユーザーの発話終了を意味すると仮定することです。</p>

          <p>音声AI エージェントのパイプラインは、小さく専門化された音声活動検出モデルを用いてポーズを識別します。VAD
            モデルは音声セグメントを「発話」または「非発話」に分類するように訓練されています。（これは音量レベルだけに基づいてポーズを識別しようとするよりもはるかに堅牢です。）</p>

          <p>VAD は音声AI 接続のクライアント側またはサーバー側のいずれでも実行できます。クライアント側で大幅な音声処理を行う必要がある場合は、VAD
            をクライアントで実行してそれを支援する必要があるでしょう。たとえば、組み込みデバイスでウェイクワードを識別し、フレーズの先頭でウェイクワードを検出した場合にのみ音声をネットワーク越しに送信するような場合です。<em>Hey,
              Siri …</em></p>

          <p>一般的には、VAD を音声AI エージェントの処理ループの一部として実行する方が少し簡単です。電話経由で接続するユーザーがいる場合、VAD を実行できるクライアントがないため、サーバーで実行する必要があります。
          </p>

          <p>音声AIで最も頻繁に使用される VAD モデルは <a href="https://github.com/snakers4/silero-vad" target="_blank">Silero VAD</a>
            です。このオープンソースモデルは CPU 上で効率的に動作し、複数言語をサポートし、8kHz と 16kHz の両方の音声で良好に機能し、ウェブブラウザでの使用向けに wasm
            パッケージとしても利用可能です。リアルタイムのモノラル音声ストリーム上で Silero を実行しても、通常は典型的な仮想マシン CPU コアの 1/8 未満の負荷で済みます。</p>

          <p>ターン検出アルゴリズムにはいくつかの設定パラメータがあります：</p>

          <ul class="arrow-list">
            <li>ターン終了とみなすために必要なポーズの長さ。</li>
            <li>発話開始イベントをトリガーするために必要な発話セグメントの長さ。</li>
            <li>各音声セグメントを発話と分類するための信頼度レベル。</li>
            <li>発話セグメントの最小音量。</li>
          </ul>
        </div>

        <div class="chunk-notes">
          <div class="chapter-image">
            <img src="../images/4x.svg" alt="VAD processing step" class="vad-image" width="100%">
            <p class="image-caption">ここでは音声認識の直前に実行されるように設定された音声活動検出処理ステップ</p>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <pre class="language-python"><code>
  # Pipecat's names and default values
  # for the four configurable VAD
  # parameters
  VAD_STOP_SECS = 0.8
  VAD_START_SECS = 0.2
  VAD_CONFIDENCE = 0.7
  VAD_MIN_VOLUME = 0.6
  
</code></pre>

          <p>これらのパラメータを調整することで、特定のユースケースに対するターン検出の挙動を大幅に改善できます。</p>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h3 id="push-to-talk">4.7.2 プッシュトゥトーク</h3>

          <p>ポーズに基づくターン検出の明白な問題は、人が一時的にポーズを取っていても話し終えていないことがある点です。</p>

          <p>個々の話し方のスタイルは異なります。会話の種類によっては人はより多くポーズを取ります。</p>

          <p>長いポーズ間隔を設定すると会話がぎこちなくなり、非常に悪いユーザー体験になります。しかし短いポーズ間隔だと、音声エージェントが頻繁にユーザーの発話を遮ってしまい、これもまた悪いユーザー体験になります。</p>

          <p>
            ポーズベースのターン検出の最も一般的な代替はプッシュトゥトークです。プッシュトゥトークとは、ユーザーが話し始めるときにボタンを押すか押し続け、話し終えたら再度ボタンを押すか放すことを要求する方式です。（昔ながらのトランシーバーの動作を思い浮かべてください。）
          </p>

          <p>プッシュトゥトークではターン検出は明確ですが、ユーザー体験は単に話す場合とは異なります。</p>

          <p>電話を使った音声AIエージェントではプッシュトゥトークは実現できません。</p>

        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h3 id="endpoint-markers">4.7.3 エンドポイントマーカー</h3>

          <p>特定の語をターンの終了マーカーとして使用することもできます。（CB 無線でトラッカーが "over" と言うのを思い浮かべてください。）</p>

          <p>特定のエンドポイントマーカーを識別する最も簡単な方法は、各文字起こし断片に対して正規表現マッチを実行することです。しかし、小さな言語モデルを使用してエンドポイントの単語やフレーズを検出することもできます。</p>

          <p>
            明示的なエンドポイントマーカーを使用する音声AIアプリケーションはあまり一般的ではありません。ユーザーはこれらのアプリケーションに話しかける方法を学ぶ必要があります。しかし、このアプローチは特化したユースケースでは非常によく機能することがあります。
          </p>

          <p>たとえば、昨年見た素晴らしいデモでは、ある人が副業として自分用に作ったライティングアシスタントがありました。彼らはターンの終端を示したりモードを切り替えたりするために、さまざまなコマンドフレーズを使用していました。
          </p>

        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h3 id="context-aware-turn-detection">4.7.4 コンテキストに対応したターン検出（セマンティック VAD とスマートターン）</h3>

          <p>人間がターン検出を行うとき、次のようなさまざまな手がかりを使います：</p>

          <ul class="arrow-list">
            <li>"えっと" のようなフィラー語を、発話が続く可能性が高いものとして識別すること。</li>
            <li>文法構造。</li>
            <li>電話番号の桁数のように特定のパターンを知っていること。</li>
            <li>ポーズ前に語を引き延ばすなどの抑揚や発音パターン。</li>
          </ul>

          <p>ディープラーニングモデルはパターン識別に非常に優れています。LLM
            は潜在的な文法知識を豊富に持ち、プロンプトによってフレーズのエンドポイント検出を行わせることができます。より小さく専門化された分類モデルは、言語、抑揚、発音パターンで訓練することができます。</p>

          <p>音声AIエージェントが商業的にますます重要になるにつれて、コンテキストに対応した音声AIのターン検出の新しいモデルが登場すると予想されます。</p>

          <p>主に二つのアプローチがあります：</p>

          <ol class="list-decimal">
            <li>リアルタイムで動作できる小さなターン検出モデルを訓練する。このモデルを VAD
              モデルと組み合わせて、または代替として使用します。ターン検出モデルはテキスト上でパターンマッチするように訓練できます。テキストモードのターン検出モデルは文字起こしの後にパイプライン内でインラインで動作し、効果的にするには特定の文字起こしモデルの出力で訓練する必要があります。あるいは、ターン検出モデルをネイティブに音声上で動作するように訓練することもでき、これにより言語レベルのパターンだけでなく抑揚、発話のペーシング、発音パターンを考慮した分類が可能になります。ネイティブ音声のターン検出モデルは文字起こし情報を必要としないため、文字起こしと並列して実行でき、パフォーマンスを改善できます。
            </li>
            <li>大規模 LLM と few shot プロンプトを使ってターン検出を行う。LLM
              は通常インラインで使用するには遅すぎてパイプラインをブロックしてしまいます。この問題を回避するために、パイプラインを分割してターン検出と「グリーディ」な会話推論を並列で行うことができます。</li>
          </ol>
        </div>
        <div class="chunk-notes">

        </div>
      </div>
      <div class="chunk-row">
        <div class="chunk-content">

          <pre class="language-python"><code> 
    [
      transport.input(),
      vad,
      audio_accumulater,
      ParallelPipeline(
      [
        FunctionFilter(filter=block_user_stopped_speaking),
      ],
      [
        ParallelPipeline(
        [
          classifier_llm,
          completeness_check,
        ],
        [
          tx_llm,
          user_aggregator_buffer,
        ],
        )
      ],
      [
        conversation_audio_context_assembler,
        conversation_llm,
        bot_output_gate,
      ],
      ),
      tts,
      transport.output(),
      context_aggregator.assistant(),
    ],

</code></pre>
          <p class="image-caption">Pipecat パイプラインの <a
              href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/22d-natural-conversation-gemini-audio.py"
              target="_blank">コンテキスト対応ターン検出用コード</a> は Gemini 2.0 Flash
            ネイティブオーディオ入力を使用しています。ターン検出とグリーディな会話推論は並列で動作します。ターン検出推論がフレーズのエンドポイントを検出するまで出力はゲートされます。</p>

          <p>ターン検出における最近の動向：</p>
          <ul class="arrow-list">
            <li>3 月に OpenAI は Realtime API 向けの新しいコンテキスト対応ターン検出機能をリリースしました。彼らはこの機能を、より単純な <em>サーバー
                VAD</em>（ポーズベースのターン検出）と対比して <em>セマンティック VAD</em> と呼んでいます。ドキュメントは <a
                href="https://platform.openai.com/docs/guides/realtime-vad#semantic-vad" target="_blank">こちら</a> にあります。
            </li>
            <li><a href="https://www.tavus.io" target="_blank">Tavus</a>
              はトランスフォーマーベースのネイティブ音声ターン検出モデルを開発し、現在はリアルタイム会話ビデオ API の一部となっています。Tavus チームは問題領域とモデルの動作に関する非常に良い <a
                href="https://www.tavus.io/post/sparrow-0-advancing-conversational-responsiveness-in-video-agents-with-transformer-based-turn-taking"
                target="_blank">技術的概要</a> を公開しています。</li>
            <li><a href="https://github.com/pipecat-ai/smart-turn" target="_blank">Smart Turn</a> オープンソースモデルは、Pipecat
              コミュニティによって構築・維持されている最先端のネイティブ音声ターン検出モデルです。すべての訓練データ、訓練コード、推論コード、およびモデルウェイトがオープンソースで公開されています。<sup>[30]</sup>
            </li>
          </ul>



        </div>


        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[30] <a href="https://github.com/pipecat-ai/smart-turn" target="_blank">Pipecat オープンソース smart turn
                  検出モデル</a></p>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h2 id="interruption-handling">4.8. 中断処理</h2>

          <p><em>中断処理</em>とは、ユーザーが音声AIエージェントを中断できるようにすることを指します。中断は会話の通常の一部であるため、中断を適切に扱うことが重要です。</p>

          <p>中断処理を実装するには、パイプラインのすべての部分がキャンセル可能である必要があります。また、クライアント上でオーディオ再生を非常に迅速に停止できる必要があります。</p>

          <p>
            一般に、使用しているフレームワークは中断がトリガーされたときにすべての処理を停止することを処理してくれます。<strong>しかし、リアルタイムより速い速度で生のオーディオフレームを送信するAPIを直接使用している場合は、再生を手動で停止し、オーディオバッファをフラッシュする必要があります。</strong>
          </p>

        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h3 id="avoiding-spurious-interruptions">4.8.1 誤検知による中断を避ける</h3>

          <p>意図しない中断の原因となるいくつかの要素は注意に値します。</p>

          <ol class="list-decimal">
            <li>
              音声として分類される一時的なノイズ。優れたVADモデルは「ノイズ」と音声をうまく分離します。しかし、発話の始めに現れる種々の短く鋭い音は、発話の初期段階で中程度の音声確信度が付与されることがあります。咳やキーボードのクリックはこのカテゴリに入ります。VAD
              の開始セグメント長と確信度レベルを調整することで、この種の中断を最小限に抑えることができます。トレードオフは、開始セグメント長を長くし、確度の閾値を上げると、本来検出したい非常に短いフレーズを完全な発話として検出できなくなる可能性があることです。<sup>[31]</sup>
            </li>

            <li>
              エコーキャンセレーションの失敗。エコーキャンセレーションアルゴリズムは完全ではありません。無音から音声再生への移行は特に難しい問題です。多くの音声エージェントのテストを行っていると、ボットが話し始めた直後に自分自身を中断するのを聞いたことがあるでしょう。原因は、エコーキャンセレーションが初期の音声をわずかにマイクにフィードバックさせてしまうことです。最小VAD開始セグメント長はこの問題を回避するのに役立ちます。また、音量の急激な遷移を避けるために音量レベルに対して指数平滑化<sup>[32]</sup>を適用することも有効です。
            </li>

            <li>
              バックグラウンドスピーチ。VADモデルはユーザーの発話と背景の発話を区別しません。背景の発話が音量閾値より大きいと、背景の発話が中断を引き起こします。スピーカー分離のためのオーディオ処理ステップは、背景スピーチによる誤検知中断を減らすことができます。上記の<a
                href="#server-side-noise">サーバー側ノイズ処理とスピーカー分離</a>の節の議論を参照してください。</li>
          </ol>

        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h3 id="maintaining-accurate-context">4.8.2 中断後に正確なコンテキストを維持する</h3>

          <p>LLMはリアルタイムより速く出力を生成するため、中断が発生したときにはユーザーに送信するためにLLMの出力がキューに入っていることがよくあります。</p>

          <p>通常、会話のコンテキストはユーザーが実際に聞いた内容と一致させたい（パイプラインがリアルタイムより速く生成した内容ではなく）ことが多いです。</p>

          <p>おそらく会話のコンテキストをテキストとして保存しているでしょう。<sup>[33]</sup></p>

          <p>したがって、ユーザーが実際に<em>聞いた</em>テキストが何であるかを特定する方法が必要です！</p>

          <p>
            最上級のSpeech-to-textサービスは単語レベルのタイムスタンプデータを出力します。これらの単語レベルのタイムスタンプを使用して、ユーザーが聞いたオーディオに一致するアシスタントメッセージのテキストをバッファして組み立てます。単語レベルのタイムスタンプに関する議論は上記の<a
              href="#text-to-speech">テキスト読み上げ</a>の節を参照してください。Pipecatはこれを自動的に処理します。</p>

        </div>

        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[31] Pipecatの標準パイプライン構成は、VADと文字起こしイベントを組み合わせて、誤検知中断と見逃しの両方を回避しようとします。</p>
            </div>
            <div class="footnote">
              <p>[32] <a href="https://dub.sh/voice-agents-030" target="_blank">Pipecat VAD入力オーディオ指数平滑化コード</a></p>
            </div>
            <div class="footnote">
              <p>[33] 標準のコンテキスト構造は、OpenAIによって開発されたユーザー/アシスタントのメッセージリスト形式です。</p>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h2 id="managing-conversation-context">4.9. 会話コンテキストの管理</h2>

          <p>LLMはステートレスです。つまり、対話型の会話では、各応答を生成するたびに、これまでのすべてのユーザーとエージェントのメッセージやその他の構成要素をLLMに再度渡す必要があります。</p>

          <pre><code>Turn 1:
  <strong>User:</strong> What's the capital of France?
  <strong>LLM:</strong> The capital of France is Paris.

Turn 2:
  <strong>User:</strong> What's the capital of France?
  <strong>LLM:</strong> The capital of France is Paris.
  <strong>User:</strong> Is the Eiffel Tower there?
  <strong>LLM:</strong> Yes, the Eiffel Tower is in Paris.
  
Turn 3:
  <strong>User:</strong> What's the capital of France?
  <strong>LLM:</strong> The capital of France is Paris.
  <strong>User:</strong> Is the Eiffel Tower there?
  <strong>LLM:</strong> Yes, the Eiffel Tower is in Paris.
  <strong>User:</strong> How tall is it?
  <strong>LLM:</strong> The Eiffel Tower is about 330 meters tall.
</code></pre>
          <p class="image-caption">各ターンで会話履歴全体をLLMに送信すること。</p>


          <p>各推論操作（各会話ターン）について、LLMに送信できるものは次のとおりです：</p>

          <ul class="arrow-list">
            <li>システム指示</li>
            <li>会話メッセージ</li>
            <li>LLMが使用するためのツール（Function Calling）</li>
            <li>構成パラメータ（例：temperature）</li>
          </ul>




        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h3 id="differences-between-llm-apis">4.9.1 LLM APIによる違い</h3>

          <p>この一般的な設計は、今日の主要なLLMすべてに共通しています。</p>

          <p>しかし、各プロバイダーのAPIには違いがあります。OpenAI、Google、Anthropicはそれぞれメッセージ形式、ツール/関数定義の構造、システム指示の指定方法に違いがあります。</p>

          <p>
            サードパーティのAPIゲートウェイやソフトウェアライブラリがAPI呼び出しをOpenAIの形式に変換することがあります。これは有用です。なぜなら異なるLLM間を切り替えられることは便利だからです。しかし、これらのサービスは常に違いを適切に抽象化できるわけではありません。新機能や各API固有の機能は常にサポートされるとは限りません。（そして時には変換レイヤーにバグがある場合もあります。）
          </p>

          <p>抽象化するか否かは、AIエンジニアリングのこの比較的初期の時代には依然として答えがありません。<sup>[34]</sup></p>

          <p>
            例えばPipecatは、コンテキストメッセージとツール定義の両方についてOpenAI形式へのメッセージの変換を行います。しかしこれを行うかどうか、またどのように行うかは大いにコミュニティで議論された主題でした！<sup>[35]</sup>
          </p>
        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[34] 自分へのメモ: Claudeに良いハムレットのジョークを考えてもらうよう頼むこと – 編集者より。</p>
            </div>
            <div class="footnote">
              <p>[35] こうしたトピックに興味があるなら、ぜひ<a href="https://discord.gg/pipecat" target="_blank">Pipecat
                  Discord</a>に参加して、そこでの議論に参加してください。</p>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h3 id="modifying-context-between-turns">4.9.2 ターン間でのコンテキスト修正</h3>

          <p>対話型のコンテキストを管理する必要があることは、音声AIエージェントの開発の複雑さを増します。一方で、コンテキストを遡及的に修正することは有用です。各会話ターンごとに、LLMに送信する内容を正確に決定できます。
          </p>

          <p>LLMは必ずしも完全な会話コンテキストを必要としません。コンテキストを短縮または要約することで、レイテンシーやコストを削減し、音声AIエージェントの信頼性を高めることができます。このトピックの詳細は下の<a
              href="#scripting">スクリプティングと指示の遵守</a>の節を参照してください。</p>



        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h2 id="function-calling">4.10. Function Calling</h2>

          <p>本番環境の音声AIエージェントはLLMのFunction Callingに大きく依存しています。</p>

          <p>Function Callingは以下の用途で使用されます：</p>

          <ul class="arrow-list">
            <li>情報取得を用いた検索強化生成（RAG）のための情報取得。</li>
            <li>既存のバックエンドシステムやAPIとのやり取り。</li>
            <li>電話技術スタックとの統合 — 転送、待ち行列、DTMFトーンの送信。</li>
            <li>スクリプトの実行遵守 – ワークフローの状態遷移を実装するFunction Calling。</li>
          </ul>

        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h3 id="function-calling-reliability">4.10.1 音声AIにおける Function Callingの信頼性</h3>

          <p>音声AIエージェントがますます複雑なユースケースに展開されるにつれて、Function Callingの信頼性はますます重要になっています。</p>

          <p>最先端のLLMはFunction Callingが着実に向上していますが、音声AIのユースケースはLLMのFunction Calling能力を限界まで引き伸ばす傾向があります。</p>

          <p>音声AIエージェントは以下の傾向があります：</p>

          <ul class="arrow-list">
            <li>対話型の会話で関数を使用する。対話型の会話では、各ターンでユーザーとアシスタントのメッセージが追加されるにつれてプロンプトがますます複雑になります。このプロンプトの複雑化はLLMのFunction
              Calling能力を劣化させます。</li>
            <li>複数の関数を定義する。音声AIワークフローでは、関数が5つ以上必要になることが一般的です。</li>
            <li>セッション中に関数を複数回呼び出します。</li>
          </ul>

          <p>
            私たちは主要なAIモデルのリリースを厳密にテストし、これらのモデルを訓練している人々と頻繁に話をしています。上記のすべての属性は、現世代のLLMを訓練するために使用されたデータに対していくらか範囲外であることは明らかです。
          </p>

          <p>これは、現世代のLLMは一般的なFunction
            Callingベンチマークで良好な結果を出していても、音声AIのユースケースでは苦戦することを意味します。モデルごと、同一モデルの異なる更新版ごとに、Function
            Callingの得意不得意が異なり、状況に応じて異なる種類のFunction Callingに対する性能も異なります。</p>

          <p id="function-calling-eval"><strong>音声AIエージェントを構築している場合、アプリのFunction
              Calling性能をテストするために独自の評価（eval）を開発することが重要です。下の<a href="#evals">Voice AI Evals</a>セクションを参照してください。</strong>
          </p>

        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h3 id="latency-function-calls">4.10.2 Function Callingのレイテンシ</h3>

          <p>Function Callingは4つの理由でレイテンシを追加します — 場合によっては大きなレイテンシになります：</p>

          <ol class="list-decimal">
            <li>LLMがFunction Callingが必要だと判断すると、Function
              Calling要求メッセージを出力します。あなたのコードはその特定の要求された関数に対して必要な処理を行い、その後、同じコンテキストにFunction
              Callingの結果メッセージを追加して再度推論を呼び出します。したがって、関数が呼び出されるたびに、推論呼び出しが1回ではなく2回必要になります。</li>
            <li>Function Calling要求はストリーミングできません。Function Callingを実行する前に、完全なFunction Calling要求メッセージが必要です。</li>
            <li>
              プロンプトに関数定義を追加するとレイテンシが増加する可能性があります。これはやや曖昧な点で、プロンプトに関数定義を追加したことによる追加レイテンシを測定するためのレイテンシ指向の評価（eval）を開発するのが良いでしょう。しかし、少なくとも一部のAPIでは、関数（ツール）使用が有効な場合、関数が実際に呼び出されているかどうかにかかわらず中央値のTTFT（初回トークン到着時間）が高くなることは明らかです。
            </li>
            <li>あなたの関数自体が遅い可能性があります！レガシーなバックエンドシステムと連携している場合、関数が返るのに時間がかかることがあります。</li>
          </ol>

          <p>ユーザーが話し終えたときには比較的迅速な音声フィードバックを提供する必要があります。Function
            Callingが返るのに時間がかかる可能性があると分かっている場合は、何が起きているかをユーザーに伝えて待つよう促す音声を出力することを検討してください。</p>

        </div>
        <div class="chunk-notes">
          <div class="chapter-image-positioned" data-align-with="function-calling-eval">
            <img src="../images/4ad.svg" alt="TTFT for inference that includes a function call" class="latency-image"
              width="100%">
            <p class="image-caption">Function Callingを含む推論のTTFT。LLMのTTFTは450msでスループットは毎秒100トークンとする。Function
              Calling要求のチャンクが100トークンであれば、Function
              Calling要求を出力するのに1秒かかります。次に関数を実行し再度推論を行います。今回は出力をストリームできるため、450ms後に使用可能な最初のトークンが得られます。完全な推論のTTFTは1450msです（関数自体の実行時間は含みません）。
            </p>
          </div>
        </div>
      </div>
      <div class="chunk-row">
        <div class="chunk-content">

          <p>次のいずれかを行うことができます：</p>

          <ul class="arrow-list">
            <li>Function Callingを実行する前に必ずメッセージを出力する。「Xを実行しますので少々お待ちください…」</li>
            <li>ウォッチドッグタイマーを設定し、Function Callingループがタイマー発火前に完了していない場合のみメッセージを出力する。「まだ処理中です。もう少しだけお待ちください…」</li>
          </ul>

          <p>もちろん両方行うこともできます。長時間実行されるFunction Calling中にバックグラウンドミュージックを再生することも可能です。<sup>[36]</sup></p>
        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[36] ただし、Jeopardyのテーマ曲はやめてください。</p>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h3 id="handling-interruptions">4.10.3 中断の処理</h3>

          <p>LLMはFunction Calling要求メッセージとFunction Calling応答メッセージが対になって現れることを期待して訓練されています。</p>

          <p>これは次のことを意味します：</p>

          <ol class="list-decimal">
            <li>すべてのFunction Callingが完了するまで、speech-to-speechへの推論ループを停止する必要があります。<a
                href="#async-function-calls">非同期Function Calling</a>に関する注意事項は下を参照してください。</li>
            <li>Function Callingが中断されて二度と完了しない場合、何らかのことを示すFunction Calling応答メッセージをコンテキストに入れる必要があります。</li>
          </ol>

          <p><strong>ここでのルールは、LLMが関数を呼び出した場合、リクエスト/レスポンスのメッセージペアをコンテキストに入れる必要があるということです。</strong></p>

          <ul class="arrow-list">
            <li>未解決のFunction
              Calling要求メッセージをコンテキストに入れたままマルチターン会話を続けると、LLMが訓練された方法から逸脱したコンテキストを作成することになります。（一部のAPIではこれを許可しません。）</li>
            <li>リクエスト/レスポンスのペアをまったくコンテキストに入れない場合、あなたはLLMに（in‑context
              learningを通じて）関数を呼び出さないよう教えていることになります。<sup>[37]</sup> これも結果は予測不能で、おそらく望ましくないでしょう。</li>
          </ul>
        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[37] 論文「<a href="https://arxiv.org/abs/2005.14165" target="_blank">Language Models are Few-Shot
                  Learners</a>」を参照してください。</p>
            </div>
          </div>
        </div>
      </div>
      <div class="chunk-row">
        <div class="chunk-content">
          <p>Pipecatは、Function
            Callingが開始されるたびにリクエスト/レスポンスのメッセージペアをコンテキストに挿入することで、これらのコンテキスト管理ルールの遵守を支援します。（もちろん、この挙動を上書きしてFunction
            Callingコンテキストメッセージを直接管理することもできます。）</p>

          <p>実行完了（run-to-completion）と中断可能（interruptible）の2つの異なる方法で構成されたFunction Callingについて、パターンは次のようになります。</p>

          <pre><code><strong>User:</strong>  Please look up the price of 1000 widgets.
<strong>LLM:</strong> Please wait while I look up the price for 1000 widgets. 
<strong>function call request:</strong> { name: "price_lookup", args: { item: "widget", quantity: 1000 } }
<strong>function call response:</strong> { status: IN_PROGRESS }
</code></pre>
          <p class="image-caption">初期コンテキストメッセージ。Function Calling要求メッセージとFunction Calling応答のプレースホルダー。</p>

          <pre><code><strong>User:</strong>  Please look up the price of 1000 widgets.
<strong>function call request:</strong> { name: "price_lookup", args: { item: "widget", quantity: 1000 } }
<strong>function call response:</strong> { result: { price: 12.35 } }
</code></pre>
          <p class="image-caption">Function Callingが完了したときのコンテキスト。</p>

          <pre><code><strong>User:</strong>  Please look up the price of 1000 widgets.
<strong>LLM:</strong> Please wait while I look up the price for 1000 widgets. 
<strong>function call request:</strong> { name: "price_lookup", args: { item: "widget", quantity: 1000 } }
<strong>function call response:</strong> { status: IN_PROGRESS }

<strong>User:</strong> Please lookup the price of 1000 pre-assembled modules.
<strong>LLM:</strong> Please wait while I also look up the price for 1000 pre-assembled modules. 
<strong>function call request:</strong> { name: "price_lookup", args: { item: "pre_assembled_module", quantity: 1000 } }
<strong>function call response:</strong> { status: IN_PROGRESS }
</code></pre>
          <p class="image-caption">プレースホルダーにより、Function Callingが実行されている間に会話を続けることができ、LLMを「混乱」させることを避けられます。</p>

          <pre><code><strong>User:</strong>  "Please look up the price of 1000 widgets."
<strong>LLM:</strong> "Please wait while I look up the price for 1000 widgets." 
<strong>function call request:</strong> { name: "price_lookup", args: { item: "widget", quantity: 1000 } }
<span class="pre-highlight"><strong>function call response:</strong> { status: CANCELLED }</span>

<strong>User:</strong> Please lookup the price of 1000 pre-assembled modules.
<strong>LLM:</strong> Please wait while I look up the price for 1000 pre-assembled modules.
<strong>function call request:</strong> { name: "price_lookup", args: { item: "pre_assembled_module", quantity: 1000 } }
<strong>function call response:</strong> { status: IN_PROGRESS }
</code></pre>
          <p class="image-caption">Function Callingが<strong>中断可能</strong>として構成されている場合、Function
            Calling中にユーザーが発話するとキャンセルされます。</p>



        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h3 id="streaming-mode">4.10.4 ストリーミングモードとFunction Callingのコンテンツチャンク</h3>

          <p>
            音声AIエージェントの実装では、ほとんどの場合、会話の推論呼び出しをストリーミングモードで実行します。これは初期の数個のコンテンツチャンクをできるだけ早く取得するためで、speech-to-speechへの応答レイテンシにとって重要です。
          </p>

          <p>ただし、ストリーミングモードとFunction Callingの組み合わせは扱いにくいです。ストリーミングはFunction Callingチャンクには役立ちません。LLMの完全なFunction
            Calling要求メッセージを組み立てるまで関数を呼び出すことはできません。<sup>[38]</sup></p>
        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[38] AIフレームワークを使用している場合、フレームワークがこの複雑さを隠してくれている可能性があります。</p>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">

          <p>推論プロバイダがAPIを進化させる際のフィードバックとして：Function
            Callingチャンクを原子性を持って提供し、ストリームされたコンテンツチャンクから分離するモードを提供してください。これにより、LLMプロバイダAPIを使用するコードの複雑さが大幅に軽減されます。</p>

        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h3 id="execute-function-calls">4.10.5 Function Callingをどのように、どこで実行するか</h3>

          <p>LLMがFunction Calling要求を出力したとき、あなたは何をしますか？ここに一般的に使われるパターンを示します：</p>

          <ul class="arrow-list">
            <li>要求された関数と同じ名前の関数を自分のコード内で直接実行する。これはほとんどすべてのLLM Function Callingドキュメントの例で見られる方法です。</li>
            <li>引数とコンテキストに基づいて要求を操作（オペレーション）にマップする。これは、LLMに対して汎用的なFunction
              Callingを行い、それをあなたのコード内で曖昧性を解消するように考えてください。このパターンの利点は、選択可能な関数の数を少なくすると一般にLLMがFunction
              Callingをより上手く行うことが多い点です。<sup>[39]</sup></li>
            <li>Function Callingをクライアントにプロキシする。このパターンは（電話ではなく）アプリケーション文脈で利用可能です。例えば get_location()
              関数を想像してください。ユーザーのデバイスの現在位置が必要であれば、そのデバイス上のジオロケーションAPIにフックする必要があります。</li>
            <li>Function Callingをネットワークエンドポイントにプロキシする。これはエンタープライズ用途で特に有用なパターンです。内部APIと連携する一連の関数を定義し、これらのFunction
              CallingをHTTPリクエストとして実行する抽象化をコード内に作成します。</li>
          </ul>
        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[39] ここではFunction Callingを広義のカテゴリとして考えてください —
                口語的ではなく形式的な意味での関数です。ルックアップテーブルから値を返すこともできます。SQLクエリを実行することもできます。</p>
            </div>
          </div>
          <div class="chapter-image">
            <img src="../images/4ae.svg" width="100%">
            <p class="image-caption">Function Callingパターン</p>
          </div>
        </div>

      </div>


      <div class="chunk-row">
        <div class="chunk-content">
          <h3 id="async-function-calls">4.10.6 非同期Function Calling</h3>

          <p>場合によっては、Function
            Callingからすぐに戻りたくないことがあります。関数が完了するまでの時間が予測できない場合や、まったく完了しないかもしれない場合があります。あるいは、時間とともに情報を追加できる長時間実行プロセスを起動したい場合もあります。
          </p>

          <p>
            例えば、ユーザーがツアー中に見かけるかもしれない事柄への関心を表現できるウォーキングツアーアプリを想像してください。「有名な作家が住んでいた場所があれば、特にそれについて聞きたいです。」この場合の良いアーキテクチャの一つは、ユーザーが特定の関心を表明するたびにLLMが関数を呼び出すことです。その関数はバックグラウンドプロセスを開始し、関心に関連するものが見つかるたびに情報をコンテキストに注入します。
          </p>

          <p><strong>現在のところ、LLMのFunction Callingを使ってこれを直接行うことはできません。Function
              Callingの要求/応答メッセージはコンテキスト内で一緒に現れなければなりません。</strong></p>

          <p>したがって、次の形の関数を定義する代わりに：</p>

          <ul class="arrow-list">
            <li>
              <pre><code>register_interest_generator(interest: string) -&gt; Iterator[Message]</code></pre>
            </li>
          </ul>

          <p>次のようにする必要があります：</p>

          <ul class="arrow-list">
            <li>
              <pre><code>create_interest_task_and_return_success_immediately
  (interest: string, context_queue_callback: Callable[Message]) -&gt; 
    Literal["in_progress", "canceled", "success", "failure"]</code></pre>
            </li>
          </ul>

          <p>このトピックの詳細な議論については、以下の <a href="#async-inference-tasks">Performing async inference tasks</a> セクションを参照してください。
          </p>

          <p>LLM や API がマルチモーダルな会話ユースケースをよりよくサポートするように進化するにつれて、非同期関数やジェネレータとして機能する長時間実行される関数に関するアイデアを LLM
            研究者が検討することを期待しています。</p>

        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h3 id="parallel-composite-function-calling">4.10.7 並列および複合Function Calling</h3>

          <p><em>並列Function Calling</em> は、LLM が単一の推論レスポンス内で複数のFunction Callingを要求できることを意味します。<em>複合Function
              Calling</em> は、LLM が複数の関数を柔軟に順番に呼び出し、複雑な操作を実行するために関数を連鎖させることを意味します。</p>

          <p>これらはエキサイティングな機能です！</p>

          <p>しかし、それらは音声エージェントの動作の変動性を増加させます。つまり、並列および複合Function Callingが実際の会話で期待どおりに機能しているかをテストする評価とモニタリングを開発する必要があります。
          </p>

          <p>並列Function Callingの処理はエージェントの実装をより複雑にします。特定の用途がない限り、多くの場合、並列Function Callingを無効にすることを推奨します。</p>

          <p>複合Function Callingはうまく機能すると魔法のように感じられます。複合Function Callingの早期の興味深い例の一つは、Claude Sonnet 3.5
            がファイル名とタイムスタンプに基づいてファイルからリソースをロードするために関数を連鎖させたのを見たことでした。</p>

          <pre><code><strong>User:</strong> Claude, load the most recent picture I have of the Eiffel Tower.
<strong>function call request:</strong> &lt;list_files()&gt;
<strong>function call response:</strong> &lt;['eiffel_tower_1735838843.jpg', 'empire_state_building_1736374013.jpg', 'eiffel_tower_1737814100.jpg', 'eiffel_tower_1737609270.jpg',
'burj_khalifa_1737348929.jpg']
<strong>function call request:</strong> &lt;load_resource('eiffel_tower_1737814100.jpg')&gt;
<strong>function call response:</strong> &lt;{ 'success': 'Image loaded successfully', 'image': … }&gt;
<strong>LLM:</strong> I have loaded an image of the Eiffel Tower. The image shows the Eiffel
Tower on a cloudy day.
</code></pre>
          <p class="image-caption">LLM は2つの関数 – <strong> list_files()</strong> と <strong>load_resource()</strong> –
            をどのように連鎖させて特定の指示に応答するかを自ら見つけ出します。これら二つの関数はツールリストに記述されています。しかし、この連鎖動作はプロンプトされているわけではありません。</p>

          <p>複合Function Callingは最先端の LLM の比較的新しい機能です。性能は「でこぼこ」で、驚くほど良い一方で、苛立たしいほど一貫性に欠けます。</p>

        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h2 id="multimodality">4.11. マルチモダリティ</h2>

          <p>LLM は現在、テキストに加えて音声、画像、動画を消費・生成します。</p>

          <p>前述した <a href="#speech-to-speech">speech-to-speech models</a> について話しました。これらは音声を入力として受け取り、音声を出力として生成できるモデルです。
          </p>

          <p>最先端モデルのマルチモーダル能力は急速に進化しています。</p>

          <p>GPT-4o、Gemini Flash、Claude Sonnet
            はいずれも非常に優れた視覚能力を持ち、画像を入力として受け取ります。これらのモデルにおける視覚サポートは、画像の内容を説明したり、画像に表示されるテキストを書き起こししたりすることに重点を置いて始まりました。リリースごとに能力は拡張されます。物体のカウント、バウンディングボックスの特定、画像内のオブジェクト間の関係のより良い理解などは、より新しいリリースで利用可能な有用な能力です。
          </p>

          <p>Gemini Flash は動画入力での推論も可能で、映像トラックと音声トラックの両方を理解できます。<sup>[40]</sup></p>

          <p>興味深い新しいクラスの音声対応アプリケーションは、画面を「見る」ことができ、ローカルマシンやウェブブラウザ上でタスクを実行するのを助けるアシスタントです。多くの人が音声駆動のウェブ閲覧のための足場を構築しています。
          </p>

          <p>私たちが知っているいくつかのプログラマは、最近では入力する頻度と同じくらい話すことが多いです。音声入力を接続して Cursor や Windsurf を操作するのは比較的簡単です。<sup>[41]</sup>
            また、画面キャプチャを接続して AI プログラミングアシスタントが編集エディタ内のコード、構築中のウェブアプリの UI 状態、端末の Python
            スタックトレースなど、あなたが見ているものを正確に見るようにすることも可能です。この種の完全にマルチモーダルな AI
            プログラミングアシスタントは、この文書全体で述べてきた未来の断片のもう一つの姿のように感じられます。<sup>[42]</sup></p>
        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[40] GPT-4o と Claude
                の両方で、動画から個々のフレームを抽出し、そのフレームを画像としてコンテキストに埋め込むことで動画を処理できます。このアプローチには制約がありますが、いくつかの「動画」ユースケースではうまく機能します。
              </p>
            </div>
            <div class="footnote">
              <p>[41] 深い AI 統合とツーリングを備えた人気の新しいプログラミングエディタが二つあります。</p>
            </div>
            <div class="footnote">
              <p>[42] OpenAI Dev Day 2024 Singapore での swyx の講演、 <a href="https://dub.sh/voice-agents-040"
                  target="_blank">"Engineering AI Agents"</a> を参照してください。</p>
            </div>
          </div>
        </div>

      </div>
      <div class="chunk-row">
        <div class="chunk-content">

          <p>現在、最先端モデルはさまざまな組み合わせでマルチモダリティをサポートしています。</p>

          <ul class="arrow-list">
            <li>GPT-4o (gpt-4o-2024-08-06) はテキストと画像の入力を持ち、テキスト出力を行います。</li>
            <li>gpt-4o-audio-preview はテキストと音声の入力を持ち、テキストと音声の出力を行います。（画像入力はなし）</li>
            <li>Gemini Flash はテキスト、音声、画像、動画の入力を持ちますが、出力はテキストのみです。</li>
            <li>OpenAI の新しい speech-to-text と text-to-speech モデルは完全に操作可能で gpt-4o
              の基盤上に構築されていますが、テキストと音声の間の変換に特化しています：gpt-4o-transcribe、gpt-4o-mini-transcribe、gpt-4o-mini-tts。</li>
          </ul>

          <p>マルチモーダルサポートは急速に進化しており、上記のリストはすぐに古くなると予想されます！</p>

          <p>音声AIにとって、マルチモダリティの最大の課題は、音声や画像が多くのトークンを使用することであり、トークンが増えるほどレイテンシが高くなることです。</p>

          <table class="data-table">
            <tbody>
              <tr>
                <th>メディアの例</th>
                <th>おおよそのトークン数</th>
              </tr>
              <tr>
                <td>音声をテキスト化した場合の1分間の音声</td>
                <td>150</td>
              </tr>
              <tr>
                <td>音声としての1分間の音声</td>
                <td>2,000</td>
              </tr>
              <tr>
                <td>画像1枚</td>
                <td>250</td>
              </tr>
              <tr>
                <td>動画1分</td>
                <td>15,000</td>
              </tr>
            </tbody>
          </table>

          <p>一部のアプリケーションでは、多数の画像を扱いながら会話的なレイテンシを達成することが大きなエンジニアリング上の課題です。会話的レイテンシを達成するには、コンテキストを小さく保つか、ベンダー固有のキャッシュ API
            に依存する必要があります。画像はコンテキストに多くのトークンを追加します。</p>

          <p>
            常時稼働し、作業ループの一部としてあなたの画面を監視するパーソナルアシスタントエージェントを想像してみてください。例えば、「1時間前に読もうとしていたツイートがあったけど電話がかかってきて忘れてタブを閉じてしまった。そのツイートは何だった？」と尋ねたいかもしれません。
          </p>

          <p><em>1時間前はほぼ100万トークンに相当します。</em>
            たとえモデルがコンテキストに100万トークンを収容できたとしても<sup>[43]</sup>、そのような多数のトークンで毎ターン対話型の会話を行うコストとレイテンシは現実的ではありません。</p>
        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[43] Hello, Gemini!</p>
            </div>
          </div>
        </div>
      </div>
      <div class="chunk-row">
        <div class="chunk-content">

          <p>ビデオをテキストとして要約し、要約だけをコンテキストに保持することができます。埋め込みを計算して RAG のような検索を行うこともできます。LLM は特徴要約やFunction Callingを使用して複雑な
            RAG クエリをトリガーすることに非常に長けています。しかし、これらのアプローチはいずれもエンジニアリング的に複雑です。</p>

          <p>最終的に最も効果のある手段はコンテキストキャッシングです。すべての最先端 API
            プロバイダはキャッシングのサポートを提供しています。現時点のキャッシング機能はいずれも音声AIユースケースにとって完全ではありません。今年はマルチモーダルでマルチターンの会話ユースケースが最先端モデルの訓練者の注目を集めるにつれて、キャッシュ
            API は改善されると予想しています。</p>

        </div>

        <div class="chunk-notes">
          <div class="chunk-footnotes">
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h1 id="multiple-models">5. 複数の AI モデルの使用</h1>

          <p>今日の実運用の音声AIエージェントは複数のディープラーニングモデルを組み合わせて使用しています。<sup>[44]</sup></p>

          <p>前述のとおり、典型的な音声AIの処理ループは、ユーザーの音声を speech-to-text モデルで文字起こしし、文字起こししたテキストを LLM に渡して応答を生成し、最後に text-to-speech
            ステップを実行してエージェントの音声出力を生成します。</p>

          <p>さらに、多くの実運用の音声AIエージェントは今日、複雑で多様な方法で複数のモデルを使用しています。</p>
        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[44] OpenAI と Google のベータ版音声-to-音声 API でさえ、ターン検出を実装するために専用の VAD とノイズ抑圧モデルを使用しています。</p>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h2 id="fine-tuned-models">5.1. 複数のファインチューニング済みモデルの使用</h2>

          <p>ほとんどの音声AI エージェントは、OpenAI や Google（時には Anthropic や Meta）による SOTA<sup>[45]</sup>
            モデルを使用します。最新で最も性能の高いモデルを使用することは重要です。なぜなら音声AI のワークフローは一般にモデル能力の <em>jagged frontier</em><sup>[46]</sup>
            の端にあり、ぎりぎりの領域で動作しているからです。音声エージェントは複雑な指示に従い、人間と自然な形で自由度の高い会話に参加し、関数やツールを確実に使える必要があります。</p>
        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[45] SOTA — state of the art — は AI エンジニアリングで広く使われる用語で、おおまかに「主要なAI研究所からの最新の大型モデル」を意味します。</p>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">

          <p>
            しかし、特定のユースケースでは、会話の異なる状態ごとにモデルをファインチューニングするのが合理的な場合があります。ファインチューニングされたモデルは、特定のタスクに対しては同等以上の性能を保ちながら、より小さく、より高速で、より安価に動作させることができます。
          </p>

          <p>
            非常に大きな産業用部品カタログから部品注文を支援するエージェントを想像してください。このタスクでは、プラスチック材料、金属材料、締結具、配管、電気、保護具など、各カテゴリに焦点を当てた複数の異なるモデルを訓練するかもしれません。
          </p>

        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[46] ウォートン教授の <a href="https://x.com/emollick" target="_blank">Ethan Mollick</a> は、SOTA
                モデル能力の複雑な境界領域を表すために「jagged frontier」という用語を造りました — 場合によっては驚くほど優れていることもあれば、苛立たしいほど劣ることもあります。</p>
            </div>
          </div>
        </div>
      </div>
      <div class="chunk-row">
        <div class="chunk-content">

          <p>ファインチューニングされたモデルは一般に2つの重要なカテゴリで「学習」できます:</p>

          <ol class="list-decimal">
            <li>組み込み知識 — モデルは事実を学習できます。</li>
            <li>応答パターン — モデルはデータを変換する手法を学習できます。これには会話のパターンやフローの学習も含まれます。</li>
          </ol>

          <p>仮に産業用サプライ会社が膨大な生データを持ってるとします:</p>

          <ul class="arrow-list">
            <li>各部品のデータシート、メーカー推奨、価格、内部データを含む非常に大きなナレッジベース。</li>
            <li>テキストチャットログ、電子メールのやり取り、サポート担当者との電話会話の文字起こし。</li>
          </ul>
          <div class="chapter-image">
            <img src="../images/5a.svg" alt="Using fine-tuned models" class="chunk-image-inline" width="60%">
            <p class="image-caption">
              特定の会話トピックに対してファインチューニング済みモデルを使用する例。さまざまなアーキテクチャ上のアプローチが考えられます。この例では、各会話ターンの開始時に、ルータとなるLLMが全文脈を分類します。</p>
          </div>

          <p>この生データをファインチューニング用データセットに変換する作業は大きな仕事ですが、実行可能です。必要なデータクリーニング、データセット作成、モデル訓練、モデル評価はいずれもよく理解された問題です。</p>

          <p><strong>重要なポイント: いきなりファインチューニングに飛びつかないで — まずはプロンプトエンジニアリングから始めてください。</strong></p>

          <p>
            プロンプティングはほとんどの場合、ファインチューニングと同じタスク結果を達成できます。ファインチューニングの利点は、より小さなモデルを使えることにあり、それは推論が速くコストが低くなることに繋がります。<sup>[47]</sup>
          </p>

          <p>プロンプティングを使えば、ファインチューニングよりもはるかに簡単に始められ、はるかに速く反復できます。<sup>[48]</sup></p>

          <p>会話の異なる状態に異なるモデルを使う方法を初期探索する際、プロンプトを小さな「モデル」と考えてください。大きく文脈に特化したプロンプトを作ることで LLM に何をすべきか教えています。</p>

          <ol class="list-decimal">
            <li>組み込み知識については、ナレッジベースから情報を引き出し、検索結果を効果的なプロンプトに組み立てる検索機能を実装してください。これについては、下の <a href="#rag-memory">RAG
                とメモリ</a> セクションを参照してください。</li>
            <li>応答パターンについては、モデルに異なる質問への応答の例を埋め込みます。場合によっては数例で十分なこともあります。場合によっては、多数 — 100 を超える例が必要になることもあります。</li>
          </ol>
        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[47] プロンプティングとファインチューニングの詳細を深く掘り下げたい場合は、以下の2つの古典的な論文を参照してください: Language Models Are Few-shot Learners と
                A Comprehensive Survey of Few-shot Learning.</p>
            </div>
            <div class="footnote">
              <p>[48] 古典的なエンジニアリングアドバイスに従ってください: まず動くようにし、次に速くし、最後に安くする。プロンプトエンジニアリングからファインチューニングに移行するのは、プロセスの
                <em>速く</em> する段階の中盤あたりまで考えないでください。（そこに至ることがあるなら）
              </p>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h2 id="async-inference-tasks">5.2. 非同期推論タスクの実行</h2>

          <p>時には LLM
            を実行に比較的長い時間がかかるタスクに使いたいことがあります。コアの会話ループでは応答時間を約1秒（またはそれ以下）にすることを目指していることを忘れないでください。タスクが数秒以上かかる場合、選択肢は2つあります:
          </p>

          <ol class="list-decimal">
            <li>ユーザーに何が起きているかを伝え、待つように促す。<em>「お調べしますので少々お待ちください…」</em></li>
            <li>より長いタスクを非同期で実行し、その間会話をバックグラウンドで続けられるようにする。<em>「調べておきますね。それをしている間に他に何か質問はありますか？」</em></li>
          </ol>

          <p>非同期で推論タスクを実行する場合、その特定タスクに別の LLM を使うことを選ぶかもしれません（コアの会話ループから切り離されているため）。音声応答に許容される速度より遅い LLM
            を使ったり、特定タスク向けにファインチューニングした LLM を使うことがあります。</p>

          <p>非同期推論タスクのいくつかの例:</p>

          <ul class="arrow-list">
            <li>コンテンツの「ガードレール」を実装する。（<a href="#content-guardrails">コンテンツガードレール</a> セクション参照。）</li>
            <li>画像を作成すること。</li>
            <li>サンドボックスで実行するコードを生成すること。</li>
          </ul>

          <p>推論モデルの最近の驚異的な進歩<sup>[49]</sup> により、LLM
            に頼めることの範囲が広がりました。ただし、これらのモデルは有用な出力を出す前に思考トークンの生成にかなりの時間を費やすことが多いため、音声AI 会話ループには直接使えません。とはいえ、マルチモデルの音声AI
            アーキテクチャの非同期パートとして推論モデルを使うのは有効に機能することがあります。</p>

        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[49] 推論モデルの例としては DeepSeek R1、Gemini Flash 2.0 Thinking、OpenAI o3-mini などがあります。</p>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">

          <p>非同期推論は通常 LLM のFunction Callingでトリガーされます。単純なアプローチは2つの関数を定義することです。</p>

          <ul class="arrow-list">
            <li><code>perform_async_inference()</code> — これは長時間実行される推論タスクを実行すべきと LLM が判断したときに LLM
              によって呼び出されます。これを複数定義することもできます。注意点は、非同期タスクを開始してから直ちに基本的な <em>タスク開始に成功した</em> 応答を返す必要があることです。これはFunction
              Callingのリクエストとレスポンスのメッセージがコンテキスト内で正しい順序になるようにするためです。<sup>[50]</sup></li>
            <li><code>queue_async_context_insertion()</code> —
              これは非同期推論が終了したときにオーケストレーション層によって呼び出されます。ここでの難点は、結果をコンテキストにどのように挿入するかがやろうとしていることと、使用している LLM/API
              が許可するものに依存することです。一つの方法は、進行中の会話ターンの終わり（すべてのFunction
              Callingの完了を含む）まで待ち、非同期推論結果を特別に作成したユーザーメッセージに入れ、それからもう一度会話ターンを実行することです。</li>
          </ul>

        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[50] <a href="#async-function-calls">非同期Function Calling</a> を参照してください。</p>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h2 id="content-guardrails">5.3. コンテンツガードレール</h2>

          <p>音声AIエージェントにはいくつかの脆弱性があり、特定のユースケースで重大な問題を引き起こします。</p>

          <ul class="arrow-list">
            <li>プロンプトインジェクション</li>
            <li>幻覚（ハルシネーション）</li>
            <li>知識の陳腐化</li>
            <li>不適切または危険なコンテンツの生成</li>
          </ul>

          <p><em>コンテンツガードレール</em> はこれらを検出しようとするコードの総称であり、LLM を偶発的または悪意のあるプロンプトインジェクションから保護し、ユーザーに送信される前に不適切な LLM
            出力を検出します。</p>

          <p>ガードレールに特定のモデル（またはモデル群）を使うことにはいくつかの利点があります:</p>

          <ul class="arrow-list">
            <li>
              小型モデルはガードレールや安全性監視に適していることが多いです。問題のあるコンテンツを識別することは比較的専門的なタスクであり得ます。（実際、プロンプトインジェクションの回避については、必ずしも普通のプロンプトで応答できるモデルが望ましいわけではありません。）
            </li>
            <li>ガードレール作業に別のモデルを使う利点は、それがメインモデルとまったく同じ弱点を持たない可能性があることです。少なくとも理論上は。</li>
          </ul>

          <p>いくつかのオープンソースのエージェントフレームワークにはガードレールコンポーネントがあります。</p>

          <ul class="arrow-list">
            <li>llama-guard は Meta の <a href="https://github.com/facebookresearch/llama-stack"
                target="_blank">llama-stack</a> の一部です</li>
            <li><a href="https://github.com/NVIDIA/NeMo-Guardrails" target="_blank">NeMO Guardrails</a>
              は、LLMベースの会話アプリケーションにプログラム可能なガードレールを追加するためのオープンソースツールキットです</li>
          </ul>

          <div class="chapter-image">
            <img src="../images/5b.svg" alt="NeMo Guardrails framework" class="chunk-image-inline" width="60%">
            <p class="image-caption">NVIDIAのNeMo Guardrailsフレームワークがサポートする5種類のガードレール。図はNeMo Guardrailsのドキュメントより。</p>
          </div>
          <p>
            これら両方のフレームワークはテキストチャットを念頭に設計されており、音声AI向けではありません。しかし、どちらも有用なアイデアと抽象化を持っており、ガードレール、安全性、コンテンツモデレーションを考えるなら参考に値します。
          </p>

          <p><strong>重要なのは、LLMはここ1年でこれらの問題を回避する能力が非常に向上している点です。</strong></p>

          <p>最新の大手AI研究所のモデルでは、ハルシネーションはもはや重大な問題ではありません。現在、定期的に見られるハルシネーションは主に2種類です。</p>

          <ul class="arrow-list">
            <li>LLMが関数を呼び出すふりをするが、実際には呼び出していないケース。これはプロンプトで修正可能です。プロンプトでこの現象が起きないことを確認するには、適切な評価（evals）が必要です。評価でFunction
              Callingのハルシネーションが見られたら、再発しなくなるまでプロンプトを反復してください。（対話型の会話はLLMのFunction
              Calling能力に<em>大きな</em>負荷をかけるため、評価は実際の会話を模倣する必要があります。）</li>
            <li>ウェブ検索を期待しているときにLLMがハルシネートするケース。組み込みの検索グラウンディングはLLM
              APIの比較的新しい機能です。LLMが検索を実行するかどうかはまだやや予測しにくいです。検索を行わない場合、重みの中に埋め込まれた（古い）知識やハルシネーションで応答することがあります。Function
              Callingのハルシネーションとは異なり、プロンプトだけで簡単に修正できるものではありません。しかし、検索が実際に行われたかどうかは容易に確認できます。したがって、アプリケーションのUIにその情報を表示したり、音声会話に注入したりできます。アプリがウェブ検索に依存する場合、これを行うのは良い考えです。問題をユーザーに理解・対処させる形に移すことになりますが、ユーザーから「検索した」「検索しなかった」の区別を隠すよりは良いです。肯定的に言えば、検索が機能すると、古くなった知識の問題を大幅に解消できます。
            </li>
          </ul>

          <p>主要なラボの全てのAPIには非常に優れたコンテンツ安全フィルターが備わっています。</p>

          <p>
            プロンプトインジェクションの回避も1年前よりかなり改善されていますが、LLMが新たな機能を獲得するにつれて潜在的なプロンプトインジェクション攻撃の対象領域は拡大します。例えば、画像内テキストからのプロンプトインジェクションが今問題になっています。
          </p>

          <p>
            非常に一般的な指針として：今日の音声AIユースケースでは、通常のユーザー行動によって偶発的にプロンプトインジェクションが発生することは稀です。しかし、ユーザー入力だけでシステム命令を覆すようにLLMの挙動を操ることは確実に可能です。これを念頭にエージェントをテストすることが重要です。<strong>特に、バックエンドシステムにアクセスする任意の関数へ渡すLLM生成の入力は必ずサニタイズし、クロスチェックすることが非常に重要です。</strong>
          </p>

        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h2 id="single-inference-actions">5.4. 単一推論アクションの実行</h2>

          <p>
            AIエンジニアにとって、LLMを活用する方法を学ぶのは継続的なプロセスです。そのプロセスの一部は、これらの新しいツールに対する思考の転換です。LLMを最初に使い始めたとき、多くの人は「言語モデルは何を独自にできるのか？」という視点で考えていました。しかし、LLMは汎用ツールです。非常に広範囲の情報処理タスクに優れています。
          </p>

          <p>音声エージェントの文脈では、常にLLM推論を実行するコードパスを用意しています。LLMをコアの会話ループにのみ使う必要はありません。</p>

          <p>たとえば：</p>

          <ul class="arrow-list">
            <li>正規表現に頼ろうとするたびに、おそらくプロンプトを書けば代替できることが多いです。</li>
            <li>
              LLM出力の後処理はしばしば有用です。たとえば、UI表示用のテキストと会話用の音声という2つのフォーマットで出力を生成したい場合があります。会話用LLMに整形されたマークダウンを生成させ、次に音声生成向けに短く再フォーマットするように再度LLMにプロンプトする、ということができます。<sup>[51]</sup>
            </li>
            <li>再帰は強力です。<sup>[52]</sup> LLMにリストを生成させ、それぞれの要素に対してLLMを再度呼び出して操作を行う、といったことが可能です。</li>
            <li>マルチターン会話を要約したくなることがよくあります。LLMは素晴らしく、操作可能な要約器です。これについては以下の<a
                href="#scripting">スクリプティングと命令追従</a>のセクションで詳述します。</li>
          </ul>

        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[51] LLM出力の後処理に関しては、上記の<a href="#content-guardrails">コンテンツガードレール</a>セクションも参照してください。</p>
            </div>
            <div class="footnote">
              <p>[52] 我々はプログラマーなので、当然… — 編集部より。</p>
            </div>
          </div>
        </div>
      </div>
      <div class="chunk-row">
        <div class="chunk-content">

          <p>これらの新たに出現する実装パターンの多くは、言語モデルが自身または別の言語モデルをツールとして使うように見えます。</p>

          <p>これは非常に強力なアイデアであり、2025年には多くの人がこれに取り組むことが予想されます。エージェントフレームワークはライブラリレベルのAPIにこれを組み込むサポートを構築できます。モデルはFunction
            Callingやコード実行を行うように訓練されるのと大まかに類似した方法で、再帰的に推論を行うよう訓練され得ます。</p>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h2 id="self-improving-systems">5.5. 自己改善システムに向けて</h2>

          <p>
            SOTAの「モデル」にAPI経由でアクセスする際、我々がアクセスしているのは単一のアーティファクトではありません。APIの背後にあるシステムは、ルーティング、マルチステージ処理、分散システム技術を駆使して、高速に、柔軟に、信頼性高く、そして非常に大規模に推論を行います。これらのシステムは常に調整されます。重みが更新されます。低レベルの推論実装は常により効率的になっています。システムアーキテクチャも進化します。
          </p>

          <p>大手ラボは、ユーザーがAPIをどのように使うかと、推論や他の機能をどのように実装するかの間のフィードバックループを継続的に短縮しています。</p>

          <p>これらのますます速くなるフィードバックループは、近年の驚くべきマクロレベルのAI進展の大きな要因です。</p>

          <p>これに触発されて、我々のエージェントレベルのコードにおけるマイクロレベルのフィードバックループはどのように見えるでしょうか？会話中にエージェントの性能を改善する特定の足場（スキャフォールド）を組み込めるでしょうか？
          </p>

          <ul class="arrow-list">
            <li>エージェントがユーザーの発話が終わる前にどれくらい割り込むかを監視し、VADのタイムアウトなどのパラメータを動的に調整する。</li>
            <li>ユーザーがエージェントに割り込む頻度を監視し、LLMの応答長を動的に調整する。</li>
            <li>ユーザーが会話を理解するのに苦労していることを示すパターンを探し出す — 例えばユーザーがその言語を母国語としないスピーカーである可能性がある。会話スタイルを調整するか、言語切替を提案する。</li>
          </ul>

          <p>他に思いつくアイデアはありますか？</p>

        </div>

        <div class="chunk-notes">

          <div class="chapter-image">
            <pre><code><strong>User:</strong> How has MNI performed recently?
<strong>Agent:</strong> The Miami Dolphins won their game yesterday 21
to 3 and now lead the AFC East with two games remain-
 ing in the regular season.
<strong>User:</strong> No, I meant the stock MNI.
<strong>Agent:</strong> Ah, my apologies! You're asking about the
stock performance of MNI, which is the ticker symbol
 for McClatchy Company …
 From this point on, the model will bias towards
interpreting phonemes or transcribed text as "MNI"
 rather than "Miami".
            </code></pre>
            <p class="image-caption">マルチターンセッション中のユーザーフィードバックに基づいて挙動を調整するLLMの例（コンテキスト内学習）</p>
          </div>
          <div class="chunk-footnotes">
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h1 id="scripting">6. スクリプトと命令追従</h1>

          <p>1年前、自然な人間の待ち時間でのオープンエンドな会話が可能な音声エージェントを構築できるだけでも興奮するものでした。</p>

          <p>
            現在では、音声AIエージェントを実世界の複雑なタスクに展開しています。今日のユースケースでは、セッション中にLLMに特定の目標に集中させる指示が必要です。しばしば、LLMに特定の順序でサブタスクを実行させる必要があります。
          </p>

          <p>たとえば、医療の患者受付ワークフローでは、エージェントに次のことを行ってほしいです：</p>

          <ul class="arrow-list">
            <li>何よりも先に患者の本人確認を行うことを確認する。</li>
            <li>患者が現在服用している薬を必ず尋ねること。</li>
            <li>患者が薬Xを服用していると言った場合、特定のフォローアップ質問をすること。</li>
            <li>などなど…</li>
          </ul>

          <p>我々は、ステップごとのワークフローを作成することを <em>スクリプティング</em>
            と呼びます。過去1年の音声AI開発で得た教訓の1つは、<em>プロンプトエンジニアリング</em>だけでスクリプトの信頼性を達成するのは難しい、ということです。</p>

          <p>単一のプロンプトに詰め込める詳細には限界があります。関連して、マルチターン会話でコンテキストが大きくなると、LLMが追跡すべき情報が増え、命令追従の精度は低下します。</p>

          <p>多くの音声AI開発者は、複雑なワークフロー構築に状態機械アプローチへ移行しています。LLMを導くために長く詳細なシステム命令を書く代わりに、一連の状態を定義できます。各状態は：</p>

          <ul class="arrow-list">
            <li>システム命令とツールのリスト。</li>
            <li>会話コンテキスト。</li>
            <li>現在の状態から別の状態への一つ以上の移動手段。</li>
          </ul>

          <p>各状態遷移は以下を行う機会です：</p>

          <ul class="arrow-list">
            <li>システム命令とツールのリストを更新する。</li>
            <li>コンテキストを要約または修正する。<sup>[53]</sup></li>
          </ul>
        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[53] 通常は、コンテキスト要約を実行するためにLLM推論呼び出しを行います。:-)</p>
            </div>
          </div>
        </div>
      </div>
      <div class="chunk-row">
        <div class="chunk-content">

          <p>状態機械アプローチは、より短く焦点を絞ったシステム指示、ツール一覧、およびコンテキストがLLMの指示遵守を大幅に改善するため、うまく機能します。</p>

          <p>課題は、一方でLLMの自由で自然な会話能力を活用することと、他方でLLMがやるべき重要な部分を確実に実行するようにすることの間で、適切なバランスを見つけることです。</p>

          <p><a href="https://github.com/pipecat-ai/pipecat-flows" target="_blank">Pipecat Flows</a>
            は、開発者がワークフローステートマシンを作成するのを支援する、Pipecatの上に構築されたライブラリです。</p>

          <p>ステート図はJSONとして表現され、Pipecatプロセスにロードできます。これらのJSONステート図を作成するためのグラフィカルエディタがあります。</p>
          <div class="chapter-image">
            <img src="../images/6a.png" alt="Pipecat Flows graphical editor" class="chunk-image-inline" width="80%">
            <p class="image-caption">Pipecat Flows グラフィカルエディタ</p>
          </div>
          <p>Pipecat Flows とステートマシンは現在多くの開発者に採用されています。しかし、複雑なワークフローの抽象化を構築するための他の興味深い考え方もあります。</p>

          <p>AI研究開発の活発な分野の一つはマルチエージェントシステムです。ワークフローを通過すべき一連の状態として考えるのではなく、マルチエージェントシステムとして考えることもできます。</p>

          <p>
            Pipecatのコアとなるアーキテクチャ要素の一つは並列パイプラインです。並列パイプラインを使うと、処理グラフを通るデータを分割して二度（あるいはそれ以上）処理できます。データをブロックしたりフィルタリングしたりできます。多数の並列パイプラインを定義できます。ワークフローをゲート付きで調整された並列パイプラインの集合として考えることもできます。
          </p>

          <p>音声AIツールの急速な進化は刺激的であり、こうした新しい種類のプログラムを構築するための最良の方法を見つける段階がまだ初期であることを強調しています。</p>

        </div>

        <div class="chunk-notes">

          <div class="chunk-footnotes">

          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h1 id="evals">7. 音声AIの評価（Evals）</h1>

          <p>非常に重要なツールの一種が eval、すなわち評価です。</p>

          <p><em>Eval</em> は、システムの能力を評価し品質を判断するツールやプロセスを指す機械学習用語です。</p>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h2 id="evals-different">7.1. 音声AIの評価はソフトウェアのユニットテストとは異なる</h2>

          <p>従来のソフトウェアエンジニアリング出身であれば、テストを（主に）決定論的な作業として考える習慣があるでしょう。</p>

          <p>音声AIは従来のソフトウェア工学とは異なるテストを必要とします。音声AIの出力は非決定論的です。音声AIのテスト入力は複雑で分岐するマルチターンの会話です。</p>

          <p>特定の入力が特定の出力を生成することをテストする代わりに <code>(f(x) = y)</code>、確率的な評価を実行する必要があります —
            ある種のイベントがどのくらいの頻度で発生するかを見るために多数のテストランを行います<sup>[54]</sup>。あるテストではケースのクラスを10回中8回正しく処理できれば受容可能で、他のテストでは精度が9.99/10である必要があります。
          </p>
        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[54] ユーザー要求が満たされた、エージェントがユーザーの発話を遮った、エージェントが話題から逸れた、など</p>
            </div>
          </div>
        </div>
      </div>
      <div class="chunk-row">
        <div class="chunk-content">

          <p>単一の入力だけがあるのではなく、多数の入力、すなわちすべてのユーザー応答があることになります。これにより、ユーザー行動をシミュレートしようとしない限り、音声AIアプリケーションのテストが非常に困難になります。
          </p>

          <p>最後に、音声AIのテストは二値的な結果を持たず、伝統的なユニットテストのように明確な✅や❌を返すことは稀です。代わりに、結果をレビューしてトレードオフを判断する必要があります。</p>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h2 id="failure-modes">7.2. 失敗モード</h2>

          <p>
            音声AIアプリには特有の形態や失敗モードがあり、評価の設計と実行に影響を与えます。レイテンシは重要です（テキストモードでは許容されるレイテンシが音声システムでは失敗になります）。マルチモデル構成です。（例えば、性能の低下はLLMの挙動ではなくTTSの不安定さが原因である可能性があります）。
          </p>

          <p>今日しばしば課題となる分野には次のようなものがあります：</p>

          <ul class="arrow-list">
            <li>最初の発話までの時間やエージェント応答までのレイテンシ</li>
            <li>文字起こしの誤り</li>
            <li>住所、メール、名前、電話番号の理解と発話</li>
            <li>割り込み（途中遮断）</li>
          </ul>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h2 id="eval-strategy">7.3. 評価戦略の策定</h2>

          <p>基本的な評価プロセスは、プロンプトとテストケースを記載したスプレッドシートほど簡単なものでも構いません。</p>

          <p>典型的なアプローチの一つは、新しいモデルをテストしたりシステムの主要部分を変更したりするたびに各プロンプトを実行し、LLMを使って応答が期待されるパラメータの定義内に収まるかを判定することです。</p>

          <p>基本的な評価を持つことは何も持たないよりずっと良いですが、評価に投資し、非常に良い評価を持つことは、規模を運用し始めると重要になります。</p>

        </div>

        <div class="chunk-notes">

          <div class="chunk-footnotes">
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <p>音声AIユースケース向けの高度なツールを提供する評価プラットフォームは出始めたばかりです。音声（オーディオ）評価の特定ワークフローとツールに早期投資しているプラットフォームとしては、<a
              href="https://coval.dev" target="_blank">Coval</a>、<a href="https://freeplay.ai/"
              target="_blank">FreePlay</a>、および <a href="https://wandb.ai/site/weave/" target="_blank">Weights &amp;
              Biases Weave</a> の三つがあります。いずれもPipecatとの統合が良好です。</p>
          <div class="chapter-image">
            <img src="../images/7a.jpg" alt="A screenshot from the Coval evals platform UI" class="chunk-image-inline"
              width="80%">
            <p class="image-caption">Coval evals プラットフォームUIのスクリーンショット</p>
          </div>

          <p>これらのプラットフォームは次の点で役立ちます：</p>

          <ul class="arrow-list">
            <li>プロンプトの反復改善（プロンプトイテレーション）。</li>
            <li>オーディオ、ワークフロー、Function Calling、会話の意味的評価に関する既製の指標。</li>
            <li>問題領域のヒルクライミング（例えば、エージェントを割り込み対応でより良くする）。</li>
            <li>回帰テスト（ある問題領域を修正したときに、以前に解決された他の領域での回帰を導入していないことを確認する）。</li>
            <li>開発者による変更やユーザーコホート間での、時間経過に伴う性能変化の追跡。</li>
          </ul>

        </div>

        <div class="chunk-notes">
          <div class="chunk-footnotes">

          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h1 id="telephony">8. 電話インフラストラクチャとの統合</h1>

          <p><strong>今日最も急成長している音声AIサービスの多くは電話通話に関係しています。</strong>現在、大量のAI音声エージェントが電話に応答し、発信を行っています。</p>

          <p>
            この一部は従来のコールセンターで起きています。コールセンターは主に音声AIを「ディフレクション率（自動化で処理できる通話の割合）」を改善する技術と見なしています。これにより音声AI導入のROIが明確になります。LLMの1分あたりコストが人間エージェントの1分あたりコストより安ければ、導入判断は容易です<sup>[55]</sup>
          </p>
        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[55] もちろん、AIエージェントの性能が良好であることが前提です。今日の多様なカスタマーサポートユースケースの多くでは、その条件が満たされています。</p>
            </div>
          </div>
        </div>
      </div>
      <div class="chunk-row">
        <div class="chunk-content">

          <p>ただし、単純なROI計算を超えて採用を加速するいくつかの興味深い動きが起きています。</p>

          <p>音声AIエージェントは人間のスタッフとは異なる方法でスケーラブルです。一度音声AIを導入すると、ピーク時の待ち時間が短くなります。（その結果として顧客満足度スコアが上がります。）</p>

          <p>
            また、LLMにより良いツールを与えることで、LLMが人間エージェントより良い仕事をすることがあります。多くのカスタマーサポート状況では、人間のエージェントは複数のレガシーなバックエンドシステムを扱わなければなりません。適時に情報を見つけることが課題になります。同じ状況に音声AIを導入する際には、これらのレガシーシステムへのAPIレベルのアクセスを構築する必要があります。新しいLLM＋APIレイヤーが音声AIへの技術移行を可能にしています。
          </p>

          <p>生成AIが今後数年でコールセンターの景観を完全に再構築することは明らかです。</p>

          <p>
            コールセンター以外でも、音声AIは小規模ビジネスが電話応対を行う方法や、電話を情報探索と調整の手段として使う方法を変えています。我々は毎日、あらゆる業界向けの専門的なAI電話ソリューションを構築するスタートアップと話をしています。
          </p>

          <p>この分野の人々はよく冗談めかして、やがて人間は電話をかけたり受けたりしなくなるだろうと言います。電話はすべてAI対AIになるだろうと。私たちが見るトレンドラインからすると、その言葉には一定の真実があります！</p>

          <p>音声AIのための電話技術に関心があるなら、いくつかの頭字語や共通の概念を知っておくとよいです。</p>

          <ul class="arrow-list">
            <li>PSTNは<em>public, switched, telephone
                network</em>（公衆交換電話網）です。実際の電話番号を持つ実際の電話とやり取りする必要がある場合、PSTNプラットフォームと連携する必要があります。Twilioはほとんどの開発者が聞いたことのあるPSTNプラットフォームです。
            </li>

            <li>SIP は IP 電話に使用される特定のプロトコルですが、一般的な意味ではシステム間の電話インターコネクトを指すために SIP と呼ばれます。たとえばコールセンターの技術スタックと連携する場合は SIP
              を使用する必要があります。SIP プロバイダと契約するか、独自の SIP サーバーをホストすることができます。</li>

            <li>DTMF トーンは電話メニューを操作するためのキー押下音です。音声エージェントは実世界の電話システムと対話するために DTMF トーンを送信できる必要があります。LLM
              は電話ツリーの処理がかなり得意です。少しプロンプトエンジニアリングを行い、DTMF トーンを送信する関数を定義すれば対応できます。</li>

            <li>音声エージェントはしばしば通話転送を実行する必要があります。単純な転送では、音声AIが関数を呼び出して通話転送をトリガーすることでセッションを終了します。<sup>[56]</sup>
              <em>ウォームトランスファー</em>
              は、エージェント同士が通話者を第二のエージェントに接続する前に互いに会話する形での引き継ぎです。音声AIエージェントは人間と同様にウォームトランスファーを行うことができます。音声エージェントは最初に人間の通話者と話し、次に通話者を保留にして新しい人間エージェントと会話を行い、その後通話者をその人間エージェントにつなぎます。
            </li>
          </ul>

        </div>

        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[56] 実際の転送操作は、テレフォニー プラットフォームへの API 呼び出しか、SIP REFER アクションである可能性があります。</p>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h1 id="rag-memory">9. RAG とメモリ</h1>

          <p>音声AIエージェントはしばしば外部システムから情報にアクセスします。たとえば、次のようなことが必要になるかもしれません：</p>

          <ul class="arrow-list">
            <li>ユーザーに関する情報を LLM のシステム指示に組み込む。</li>
            <li>過去の会話履歴を取得する。</li>
            <li>ナレッジベースで情報を参照する。</li>
            <li>ウェブ検索を行う。</li>
            <li>リアルタイムの在庫や注文状況の確認を行う。</li>
          </ul>

          <p>これらはすべて RAG（Retrieval Augmented Generation：検索強化生成）のカテゴリに入ります。RAG は情報検索と LLM プロンプトを組み合わせることを指す一般的な AI
            エンジニアリング用語です。</p>

          <p>音声エージェントにおける「最も単純な RAG」は、会話開始前にユーザー情報を参照し、その情報を LLM のシステム指示に統合することです。</p>

          <pre><code>user_info = fetch_user_info(user_id)

system_prompt_base = "You are a voice AI assistant..."

system_prompt = (
  system_prompt_base
  + f"""
The name of the patient is {user_info["name"]}.
The patient is {user_info["age"]} years old.
The patient has the following medical history: {user_info["summarized_history"]}.
"""
)
</code></pre>
          <p class="image-caption">単純な RAG – セッション開始時にルックアップを実行する</p>

          <p>RAG は奥が深く、急速に変化している分野です。<sup>[57]</sup>
            手法は、上記のような基本的なルックアップと文字列挿入を行う比較的単純なアプローチから、埋め込みとベクトルデータベースを使って非常に大量の半構造化データを整理するシステムまで多岐に渡ります。</p>

        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[57] うーん。最近の生成 AI の他の分野と同じように聞こえますね。</p>
            </div>
          </div>
        </div>
      </div>
      <div class="chunk-row">
        <div class="chunk-content">

          <p>多くの場合、80/20 のアプローチで大きな効果が得られます。既存のナレッジベースがあるなら、すでにある API
            を使いましょう。検索した結果を会話コンテキストに注入するいくつかのフォーマットをテストするための簡単な評価を作成します。本番にデプロイして、実際のユーザーでどの程度うまく機能するかを監視します。</p>

          <pre><code>async def query_order_system(function_name, tool_call_id, args, llm, context, result_callback):
  "First push a speech frame. This is handy when the LLM response might take a while."
  await llm.push_frame(TTSSpeakFrame("Please hold on while I look that order up for you."))

  query_result = order_system.get(args["query"])
  await result_callback({
    "info": json.dumps({
     "lookup_success": True,
     "order_status": query_result["order_status"],
     "delivery_date": query_result["delivery_date"],
    })
 })

llm.register_function("query_order_system", query_order_system)
</code></pre>
          <p class="image-caption">セッション中の RAG。情報参照が必要なときに LLM
            が呼び出せる関数を定義します。この例では、システムが応答に数秒かかることをユーザーに伝えるために、あらかじめ設定した発話フレーズも出力しています。</p>

          <p>いつものように、音声AIではレイテンシが非音声システムより大きな課題になります。LLM がFunction
            Callingリクエストを行うと追加の推論呼び出しがレイテンシに加わります。外部システムでの情報参照も遅くなる可能性があります。作業中であることを知らせるために、RAG
            ルックアップを実行する前に簡単な音声出力をトリガーするのが有用なことが多いです。</p>

          <p>より広く見れば、セッション間のメモリは有用な機能です。あなたが話したことをすべて覚えておく必要がある音声AI個人アシスタントを想像してください。一般的には次のような二つのアプローチがあります：</p>

          <ol class="list-decimal">
            <li>
              各会話を永続ストレージに保存する。会話をコンテキストに読み込むためのいくつかのアプローチをテストする。たとえば、パーソナルアシスタントのユースケースにうまく機能する戦略は次のようなものです：エージェント起動時に最新の会話を常に完全に読み込み、最新
              N 件の会話の要約を読み込み、必要に応じて LLM が古い会話を動的に読み込めるようにルックアップ関数を定義する。</li>

            <li>
              会話履歴の各メッセージをメッセージグラフに関するメタデータとともにデータベースに個別に保存する。すべてのメッセージをインデックス化する（おそらくセマンティック埋め込みを使用）。これにより、分岐する会話履歴を動的に構築できます。アプリが画像入力（LLM
              Vison）を多用する場合はこれを検討したいかもしれません。画像はコンテキスト空間を多く消費します！<sup>[58]</sup> このアプローチは分岐型 UI
              を構築することも可能にし、AIアプリ設計者がこれから探求し始めている方向の一つです。</li>
          </ol>

        </div>

        <div class="chunk-notes">
          <div class="chunk-footnotes">

            <div class="footnote">
              <p>[58] 「<a href="#multimodality">マルチモーダリティ</a>」を参照してください。</p>
            </div>
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h1 id="hosting">10. ホスティングとスケーリング</h1>
          <p>音声AIアプリケーションには従来型のアプリコンポーネント（ウェブアプリのフロントエンド、API
            エンドポイント、その他のバックエンド要素）が含まれることが多いです。しかしエージェントプロセス自体は従来のアプリコンポーネントとは十分に異なるため、音声AIのデプロイとスケーリングには固有の課題があります。</p>

          <h2 id="hosting-architecture">10.1 アーキテクチャ</h2>

          <ul class="arrow-list">
            <li>音声AIエージェントの会話ループは通常長時間実行されるプロセスです（単一の応答生成が終了したら終了するリクエスト/レスポンス関数ではありません）。</li>
            <li>音声エージェントはリアルタイムでオーディオをストリームします。ストリーミングが滞るとオーディオのグリッチが生じます。（共有仮想マシンでの CPU スパイク、オーディオスレッドの実行を 10ms
              でもブロックするようなプログラムフローなど）</li>
            <li>音声エージェントは通常 WebSocket または WebRTC のいずれかの接続を必要とします。クラウドサービスのネットワークゲートウェイやルーティング製品は HTTP をサポートするほど
              WebSocket をうまくサポートしていないことが多いです。UDP をまったくサポートしていないこともあります。（WebRTC には UDP が必要です。）</li>
          </ul>

          <p>これらすべての理由から、音声AIにサーバーレスフレームワーク（AWS Lambda や Google Cloud Run など）を使用するのは一般的に不可能です。</p>

          <p>現在のベストプラクティスは次のとおりです：</p>

          <ul class="arrow-list">
            <li>プロトタイピング段階を突破したら、エンジニアリング時間を投資してエージェントをデプロイするための軽量ツールを作成し、Docker（または類似）コンテナをビルドするようにします。</li>
            <li>
              コンテナを選択したコンピュートプラットフォームにプッシュします。シンプルなデプロイでは固定数の仮想マシンを稼働させ続けるだけで十分です。しかしある時点で、自動スケールや新しいバージョンの優雅なデプロイ、サービスディスカバリとフェイルオーバーの実装、その他大規模運用のためのDevOps要件を満たすためにプラットフォームのツールに接続したくなるでしょう。
            </li>
            <li>Kubernetes は現在、コンテナ、デプロイ、およびスケーリングを管理するスタンダードです。Kubernetes
              は習得コストが高いですが、主要なクラウドプラットフォームのすべてでサポートされています。Kubernetes の周りには非常に大きなエコシステムがあります。</li>
            <li>ソフトウェア更新をデプロイする際には、既存の接続がセッション終了まで維持されるように長いドレイン時間を設定したいでしょう。これは Kubernetes ではそれほど難しくありませんが、詳細は k8s
              エンジンとバージョンによって異なります。</li>
            <li>
              コールドスタートは音声AIエージェントにとって問題です。接続時間が重要だからです。アイドル状態のエージェントプールを維持することが長いコールドスタートを回避する最も簡単な方法です。ワークロードがローカルで大規模モデルを実行することを要求しない場合、比較的少ない労力で高速なコンテナコールドスタートを設計できます。<sup>[59]</sup>
            </li>
          </ul>

          <p>仮想マシンスペックとコンテナの詰め込み（packing）は、本番デプロイで人々をつまずかせることがよくあります。エージェントに必要なスペックは、使用するライブラリやエージェントプロセス内でどれだけ CPU
            集約的な作業を行うかによって異なります。経験則としては、まず開発マシンでエージェントプロセスが消費する最大メモリの 2 倍の RAM を確保し、仮想マシンの CPU ごとにエージェントを 1
            つ実行することから始めると良いでしょう。<sup>[60]</sup></p>


        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[59] 大規模モデルをローカルで実行している場合、コールドスタートに関するアドバイスは本ガイドの範囲外です。GPU
                やコンテナの最適化に精通していない場合、必要なツールを自分で開発する学習曲線をたどるよりも専門家を見つけることを検討したほうが良いでしょう（少なくとも十分な規模で運用してコストを平準化できるまで）。</p>
            </div>
            <div class="footnote">
              <p>[60] コンテナランタイムがアイドルの CPU 上で新しいエージェントプロセスを起動していることを確認してください。これは常に k8s のデフォルトではありません。</p>
            </div>
          </div>
        </div>
      </div>
      <div class="chunk-row">
        <div class="chunk-content">
          <h2 id="hosting-cost">10.2 分あたりコストの計算</h2>

          <p>音声AIのコストは、使用するモデル、API、およびホスティングインフラストラクチャによって大きく異なります。コストはユースケースにも依存します。たとえば上の <a
              href="#cost-comparison">コスト比較</a> で述べたように、セッションが長くなるほど分あたりコストは一般的に高くなります。またテレフォニーは WebRTC トランスポートより高価です。
          </p>

          <p>コストは、OpenAIRealtime API のような音声対音声 API を使用する場合は分あたり $0.20 以上、ホスト型のバッテリーインクルード型エージェントプラットフォームで分あたり
            $0.10、かなり規模の大きいコスト最適化されたデプロイでは分あたり $0.02 まで幅があります。</p>

          <p>時々見られる誤りの一つは、音声と LLM API
            のコストを計算する前にエージェントホスティング自体をコスト最適化してしまうことです。<strong>一般に、エージェントプロセス自体のクラウドランタイムコストは総分あたりコストの 1% 未満です。</strong>
            vCPU あたりのエージェント同時実行数を最適化するためにエンジニアリング作業を費やす価値はほとんどありません。</p>

          <p><a href="https://www.livetok.io/cost-calculator">こちらはインタラクティブなコスト計算機</a>で、<a
              href="https://twitter.com/anarchyco">Gustavo Garcia</a> によって開発されました。</p>

          <div class="chapter-image">
            <img src="../images/livetok-cost-calculator.png" alt="An interactive cost calculator"
              class="chunk-image-inline" width="80%">
            <p class="image-caption">インタラクティブなコスト計算機</p>
          </div>

          <p>あるいは、スプレッドシートが好みなら、<a
              href="https://docs.google.com/spreadsheets/d/1-B3nv7fhwEoFmDs-phm280XK9phTep0qfyLKRNsEwVE/edit?gid=0#gid=0"
              target="_blank">ここにあるスプレッドシート</a> をコピーして分あたりコスト計算の出発点として使うことができます。</p>

          <div class="chapter-image">
            <img src="../images/Figure 3000 General Costs.png"
              alt="A spreadsheet to calculate per-minute costs for voice AI agents" class="chunk-image-inline"
              width="80%">
            <p class="image-caption">音声AIエージェントの分あたりコストを計算するためのスプレッドシート</p>
          </div>

          <p>スプレッドシートのスクリーンショットの数字は、Deepgram、GPT-4o、Cartesia を使用するセルフホスト型エージェントのものです。10 分セッションの場合、分あたりコストは約 2.5
            セントです。文字起こしと LLM 推論がそれぞれコストの約 1/4、音声生成が約半分を占めます。ホスティングはコストの 1% 未満です。</p>

          <p>
            もちろん、これはセルフホスティングの実際のコストを現実的に表したものではありません。自分ですべてのホスティングインフラを構築・維持する場合、エージェント自体に加えて多数のシステムや機能をセットアップ、スケール、保守する必要があります。
          </p>

          <ul class="arrow-list">
            <li>サービスディスカバリ</li>
            <li>ロードバランシング</li>
            <li>ロギング</li>
            <li>モニタリング</li>
            <li>帯域幅</li>
            <li>複数リージョン</li>
            <li>セキュリティ</li>
            <li>コンプライアンスおよび規制機能（たとえばデータ居住性）</li>
            <li>分析</li>
            <li>カスタマーサポート</li>
          </ul>



        </div>

      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h1 id="future">11. 2025年に来るもの</h1>

          <p>AIの成長に関連して、音声AIは2024年に大きく成長し、2025年もこの傾向が続くと予想されます。</p>

          <p></p>この音声AIに対する関心と採用の拡大は、以下の重要な領域での継続的な進歩を生み出します:<p>
          </p>

          <ul class="arrow-list">
            <li>
              すべてのモデル作成者とサービスプロバイダによるさらなるレイテンシ最適化。長い間、多くのサービス実装者やほとんどの公開ベンチマークはスループットに焦点を当ててきましたが、音声AIではトークン毎秒よりも最初のトークン到達時間（time
              to first token）をはるかに重視します。</li>
            <li>すべての非テキストモダリティがモデルとAPIに完全統合する方向への進展。</li>
            <li>テストおよび評価ツールにおける、より多くの音声特化機能。</li>
            <li>リアルタイムのマルチモーダルユースケースのニーズをサポートするコンテキストキャッシングAPI。</li>
            <li>複数プロバイダによる新しい音声AIエージェントプラットフォーム。</li>
            <li>複数プロバイダによるスSpeech-to-speechモデルAPI。</li>
            <li>文字起こし精度と音声生成品質を向上させるためにコンテキストを取り込めるコンテクスチュアルスピーチモデル。</li>
          </ul>

          <p>音声AI分野の4人の専門家による2025年の見解を聞きたい場合は、1月のサンフランシスコVoice AI Meetupのパネル記録の<a
              href="https://www.youtube.com/live/B6zTwHh-abw?t=3065s" target="_blank">54:05</a>に飛んでください。Karan Goel、Niamh
            Gavin、Shrestha
            Basu-Mallick、Swyxが来年に見られると予想する内容として、ユニバーサルメモリ、ハリウッドでのAI、モデルの模倣から理解への移行、そしてロボティクスに関する対立的な見解をそれぞれ述べています。</p>

          <p>楽しい一年になりそうです。</p>



        </div>

        <div class="chunk-notes">

          <div class="chunk-footnotes">
            <!-- No footnotes in this section -->
          </div>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h1 id="contributors">寄稿者</h1>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h2 id="lead-author">責任著者</h2>
          <p>Kwindla Hultman Kramer</p>
          <p>評価セクションの支援をしてくれたBrooke Hopkins、Llamaの性能とUltravoxに関する洞察を提供してくれたZach
            Koch、コンテクスチュアルスピーチモデルへの移行の重要性に関する注釈をくれたBrendan Iribeに感謝します。</p>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h2 id="contributing-authors">寄稿著者<sup>[61]</sup></h2>
          <p>aconchillo, markbackman, filipi87, Moishe, kwindla, kompfner, Vaibhav159, chadbailey59, jptaylor, vipyne,
            Allenmylath, TomTom101, adriancowham, imsakg, DominicStewart, marcus-daily, LewisWolfgang, mattieruth,
            golbin, adithyaxx, jamsea, vr000m, joachimchauvet, sahilsuman933, adnansiddiquei, sharvil, deshraj,
            balalofernandez, MaCaki, TheCodingLand, milo157, RJSkorski, nicougou, AngeloGiacco, kylegani, kunal-cai,
            lazeratops, EyrisCrafts, roey-priel, aashsach, jcbjoe, Dev-Khant, wg-daniel, cbrianhill, ankykong, nulyang,
            flixoflax, DANIIL0579, Antonyesk601, rahultayal22, lucasrothman, CarlKho-Minerva, 0xPatryk, pvilchez,
            pedro-a-n-moreira, RonakAgarwalVani, xtreme-sameer-vohra, shaiyon, soof-golan, yashn35, zboyles,
            balaji-atoa, eddieoz, mercuryyy, rahulunair, porcelaincode, weedge, wtlow003, zzz-heygen, adidoit, ArmanJR,
            Bnowako, chhao01, Regaddi, cyrilS-dev, DamienDeepgram, danthegoodman1, dleybz, ecdeng, gregschwartz, KevGTL,
            louisjoecodes, M1ngXU, mattmatters, MoofSoup, natestraub</p>
        </div>
      </div>

      <div class="chunk-row">
        <div class="chunk-content">
          <h2 id="design">デザイン</h2>
          <p>Sascha Mombartz</p>
          <p>Akhil K G</p>
          <h2 id="design">日本語訳</h2>
          <p><a href="https://github.com/tomoima525" target="_blank">Tomoaki Imai</a></p>

        </div>
        <div class="chunk-notes">
          <div class="chunk-footnotes">
            <div class="footnote">
              <p>[61] GitHubユーザー名、<a href="https://github.com/pipecat-ai/pipecat/graphs/contributors"
                  target="_blank">github.com/pipecat-ai/pipecat/graphs/contributors</a></p>
            </div>
          </div>
        </div>
      </div>

      <!-- More sections will be added as we process the content -->
    </main>

    <footer>
      <p>この書籍はCC0ライセンスの下で利用可能です。著者は法が許す最大限の範囲で、自身の著作物に対する著作権および関連する権利を放棄しています。本著作物は自由に使用でき、帰属表示は不要です。</p>
      <p class="github-link"><a href="https://github.com/pipecat-ai/voice-ai-primer-web"
          target="_blank">GitHub</a>でこのガイドの表示および貢献が可能です。</p>
    </footer>
  </div>

  <script src="../script/footnotes.js"></script>


</body>

</html>